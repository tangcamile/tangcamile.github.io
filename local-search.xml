<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>hexo配置更新记录</title>
    <link href="/cn/hexo_new_configeration/"/>
    <url>/cn/hexo_new_configeration/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>pycharm连接服务器</title>
    <link href="/cn//"/>
    <url>/cn//</url>
    
    <content type="html"><![CDATA[<font size=3><p>记一下pycharm连接远程服务器，步骤。</p><span id="more"></span><h2 id="一、建立远程服务器连接"><a href="#一、建立远程服务器连接" class="headerlink" title="一、建立远程服务器连接"></a>一、建立远程服务器连接</h2><h3 id="1-1-创建远程连接"><a href="#1-1-创建远程连接" class="headerlink" title="1.1 创建远程连接"></a>1.1 创建远程连接</h3><p><img src="https://s2.loli.net/2025/01/07/CpflgDc7yEtmsje.png" alt="image.png"></p><h3 id="1-2-本地-远程项目文件夹映射"><a href="#1-2-本地-远程项目文件夹映射" class="headerlink" title="1.2 本地-远程项目文件夹映射"></a>1.2 本地-远程项目文件夹映射</h3><p><img src="https://s2.loli.net/2025/01/07/INDzQwTGuBPcLdo.png" alt="image.png"></p><h3 id="1-3-验证连接是否成功-调出服务器文件目录"><a href="#1-3-验证连接是否成功-调出服务器文件目录" class="headerlink" title="1.3 验证连接是否成功(调出服务器文件目录)"></a>1.3 验证连接是否成功(调出服务器文件目录)</h3><p><img src="https://s2.loli.net/2025/01/07/15EgjS4FNiIJclZ.png" alt="image.png"></p><h2 id="二、在终端打开SSH连接"><a href="#二、在终端打开SSH连接" class="headerlink" title="二、在终端打开SSH连接"></a>二、在终端打开SSH连接</h2><p><img src="https://s2.loli.net/2025/01/07/rnsW9lGyZ6MKDIO.png" alt="image.png"></p><h2 id="三、新建虚拟环境"><a href="#三、新建虚拟环境" class="headerlink" title="三、新建虚拟环境"></a>三、新建虚拟环境</h2>    <div class="fold">      <div class="fold-title fold-info collapsed" data-toggle="collapse" href="#collapse-8f7bdefe" role="button" aria-expanded="false" aria-controls="collapse-8f7bdefe">        <div class="fold-arrow">▶</div>依据requirments安装依赖库      </div>      <div class="fold-collapse collapse" id="collapse-8f7bdefe">        <div class="fold-content">          <figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs vim">conda create --name dc_env <span class="hljs-keyword">python</span>=<span class="hljs-number">3.7</span><br><br><span class="hljs-keyword">cd</span> dc_code/<br><br><span class="hljs-keyword">ls</span> |<span class="hljs-keyword">grep</span> requirements.txt<br><br>conda activate dc_env<br><br>pip install -r requirements.txt -i https://mirrors.aliyun.<span class="hljs-keyword">com</span>/pypi/simple<br></code></pre></td></tr></table></figure>        </div>      </div>    </div><h2 id="四、本地配置python解释器"><a href="#四、本地配置python解释器" class="headerlink" title="四、本地配置python解释器"></a>四、本地配置python解释器</h2><h3 id="4-1-查询服务器某环境的python解释器路径"><a href="#4-1-查询服务器某环境的python解释器路径" class="headerlink" title="4.1 查询服务器某环境的python解释器路径"></a>4.1 查询服务器某环境的python解释器路径</h3><p><img src="https://s2.loli.net/2025/01/07/u3SUw16PA5rfiCO.png" alt="image.png"></p><h3 id="4-2-本地pycharm配置python解释器"><a href="#4-2-本地pycharm配置python解释器" class="headerlink" title="4.2 本地pycharm配置python解释器"></a>4.2 本地pycharm配置python解释器</h3><p><img src="https://s2.loli.net/2025/01/07/jTiUmEtyvZYqIC5.png" alt="image.png"></p><p><img src="https://s2.loli.net/2025/01/07/a4o8jtLOEnrVleq.png" alt="image.png"></p><p><img src="https://s2.loli.net/2025/01/07/W7Gxe5okwb6T4na.png" alt="image.png"></p><p>OK，环境配置完成，可以按照readme来测试代码了。</p><h2 id="参考博客"><a href="#参考博客" class="headerlink" title="参考博客"></a>参考博客</h2><ul><li><a href="https://blog.csdn.net/qq_43755954/article/details/143301325">pip install -r requirements.txt下载速度慢</a></li><li><a href="https://www.cnblogs.com/nickchen121/p/11107842.html">Anaconda常用命令</a></li><li><a href="https://blog.csdn.net/qq_45100200/article/details/130355935?ops_request_misc=%257B%2522request%255Fid%2522%253A%25229b52b46ffddd03caa0ba8ef28f39735a%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=9b52b46ffddd03caa0ba8ef28f39735a&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-2-130355935-null-null.142%5Ev101%5Epc_search_result_base8&utm_term=pycharm%E8%BF%9E%E6%8E%A5%E6%9C%8D%E5%8A%A1%E5%99%A8&spm=1018.2226.3001.4187">PyCharm连接远程服务器配置过程</a></li></ul></font>]]></content>
    
    
    
    <tags>
      
      <tag>python</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>注意力机制</title>
    <link href="/cn//"/>
    <url>/cn//</url>
    
    <content type="html"><![CDATA[<font size=3><p>学习注意力机制的原理和实现代码</p><span id="more"></span><h1 id="1-注意力机制"><a href="#1-注意力机制" class="headerlink" title="1.注意力机制"></a>1.注意力机制</h1><p>查询对象Query，被查询对象Value，通过去计算Q和V里的事物的<strong>相似度</strong>(更接近)，来判断哪些东西对Q来说更重要，哪些更不重要。</p><p>Q，$K &#x3D;k_1,k_2,\cdots,k_n$</p><p>通过点乘求内积的方法计算Q和K里的每一个事物的相似度，就可以拿到Q和$k_1$的相似度值$s_1$、Q和$k_2$的相似度值$s_2$、Q和$k_n$的相似度值$s_n$。计算$QK^T$后为避免输入的值差异过大导致Softmax的概率值过于极端，需要<strong>归一化</strong>，归一化的方式是<strong>除以嵌入向量维度的平方根。</strong></p><p>做一层$softmax(s_1, s_2,\cdots,s_n)$就可以得到概率$(a_1,a_2,\cdots,a_n)$，进而找到哪个事物对Q更重要。</p><p><img src="https://s2.loli.net/2024/12/18/ibh63dSvEWmJNG9.png" alt="image.png"></p><p>最后做一个汇总，拿到**经过注意力计算之后的图片$V’$**，现在这张图片中多了一些信息，多了于Q而言更重要、更不重要的信息。</p><p>$V&#x3D;(v_1, v_2, \cdots, v_n)$</p><p>$(a_1,a_2,\cdots,a_n) *+(v_1,v_2,\cdots,v_n)&#x3D;(a_1v_1+a_2v_2+\cdots+a_nv_n)$ &#x3D; V’</p><p><img src="https://s2.loli.net/2024/12/18/No5PiGVS4KZ2A9X.png" alt="image.png"></p><h1 id="2-自注意力机制-Self-attention"><a href="#2-自注意力机制-Self-attention" class="headerlink" title="2.自注意力机制(Self-attention)"></a>2.自注意力机制(Self-attention)</h1><p>在注意力机制中，一般来说Key和Value是相等的，或者一定具有某种关系。而Self-Attention中， Query、Key、Value三者是同源的，即K$\approx$V$\approx$Q，来源于同一个X。</p><p><strong>对X分别做三次线性变换，得到Query、Key、Value</strong>，通过X找到X里面的关键点。接下来的步骤和注意力机制一模一样，如图，列表示一个个X词向量，行表示分别要和句子中的每个词做一下相似度计算。</p><p>效果是：给定一个 X，通过自注意力模型，得到一个 Z，这个 Z 就是对 X 的新的表征（词向量），Z 这个词向量相比较 X 拥有了句法特征和语义特征。</p><p><img src="https://s2.loli.net/2024/12/18/mx1Rgsy7Sw9PAk2.png" alt="image.png"><br><img src="https://s2.loli.net/2024/12/18/UpFLilbYhmyWZXg.png" alt="image.png"></p><h1 id="3-多头自注意力-Multi-Head-Self-Attention"><a href="#3-多头自注意力-Multi-Head-Self-Attention" class="headerlink" title="3.多头自注意力(Multi-Head Self-Attention)"></a>3.多头自注意力(Multi-Head Self-Attention)</h1><h2 id="3-1-什么是多头"><a href="#3-1-什么是多头" class="headerlink" title="3.1 什么是多头"></a>3.1 什么是多头</h2><p>对于X,我们不是说，直接拿 X 去得到 Z，而是把 X 分成了 8 块（8 头），得到 Z0-Z7，然后把 Z0-Z7 拼接起来，再做一次线性变换（改变维度）得到 Z，使其和原来的X词向量维度一致。</p><h2 id="3-2-有什么作用"><a href="#3-2-有什么作用" class="headerlink" title="3.2 有什么作用"></a>3.2 有什么作用</h2><p>把X切成8个，这样原先在一个位置的X，去了空间上的8个位置，通过对8个点进行寻找(非线性变换，映射到更合理的空间)，找到更合适的位置。</p><p><img src="https://s2.loli.net/2024/12/18/f5kVD6dHu2UhzSn.png" alt="image.png"></p>]]></content>
    
    
    <categories>
      
      <category>metric-learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Attention mechanism</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>度量学习思路整合</title>
    <link href="/cn//"/>
    <url>/cn//</url>
    
    <content type="html"><![CDATA[<font size=3><p>few-shot learning的度量学习方法阅读与总结，目标是找到一篇文章，解决few-shot的以下问题：<br>在基类上，模型学会区分相似和不相似的实例，从而提高下游任务的性能；<br>支持集的样本过少，计算得到的类代表原型和实际期望得到的类原型有较大偏差，如何矫正<strong>有偏的类原型</strong>，进而提高模型分类精度。</p><span id="more"></span><h2 id="1-Prototype-Rectification-for-Few-Shot-Learning"><a href="#1-Prototype-Rectification-for-Few-Shot-Learning" class="headerlink" title="1.Prototype Rectification for Few-Shot Learning"></a>1.Prototype Rectification for Few-Shot Learning</h2><h3 id="1-1-基本信息"><a href="#1-1-基本信息" class="headerlink" title="1.1 基本信息"></a>1.1 基本信息</h3><ul><li><p>2020年，ECCV会议论文</p></li><li><p>无开源源码</p></li><li><p>提出了如何减少类原型的偏置、减少support和query dataset之间的分布差。</p></li></ul><h3 id="1-2-方法记录"><a href="#1-2-方法记录" class="headerlink" title="1.2 方法记录"></a>1.2 方法记录</h3><h4 id="1-2-1-intra-class-bias"><a href="#1-2-1-intra-class-bias" class="headerlink" title="1.2.1 intra-class bias"></a>1.2.1 intra-class bias</h4><p>为了减少实际计算出的原型和真实原型之间的差距，采用<strong>伪标签策略</strong>，补充<code>support set</code>的样本，由此得到更接近真实的原型。具体操作是取<code>top-k</code>个置信度的未标记样本，加上伪标签，加入<code>support set</code>一起计算类代表原型，其中为了避免伪标签错分给原型带来大的误差，使用<strong>加权和的平均作为修改的原型</strong>，权重的计算公式：样本和basic prototypes有更大的余弦相似度，则在修改的原型中有更高占比。</p><div class="note note-info">            <p>伪标签有一个使用前提，即需要一次性给出某个类的所有未标记样本，不适用于一个个给测试样本的情况。<br>在脑电身份识别上，一个人作为一个类，<strong>他的测试样本是否可以一次性获取到？</strong>，决定了伪标签是否适用。</p>          </div><h4 id="1-2-2-cross-class-bias"><a href="#1-2-2-cross-class-bias" class="headerlink" title="1.2.2 cross-class bias"></a>1.2.2 cross-class bias</h4><p>首先两个set被假设为分布在同一个domain中，但<code>support set</code>和<code>quary set</code>之间存在<strong>分布差</strong>，提出为了减少两者的分布差，可以把<code>quary set</code>朝<code>support set</code>移动。具体地，文章提出给每个标准化后的quary feature$\overline{X_q}$添加一个转换参数epilon。<br><img src="https://s2.loli.net/2024/12/09/qnV2rhwZTyjYGM8.png" alt="image.png"></p><div class="note note-info">            <p>减小分布差，脑电身份识别，support set用的是登记session的样本，quary set可能用的是另一个session的样本，由于时变性，两者的分布差肯定是存在的，甚至于同一个session，随着人状态的波动，样本之间估计也存在明显的分布差，这个方法可以一试。</p>          </div><h2 id="2-Free-Lunch-for-Few-shot-Learning-Distribution-Calibration"><a href="#2-Free-Lunch-for-Few-shot-Learning-Distribution-Calibration" class="headerlink" title="2.Free Lunch for Few-shot Learning: Distribution Calibration"></a>2.Free Lunch for Few-shot Learning: Distribution Calibration</h2><h3 id="2-1-基本信息"><a href="#2-1-基本信息" class="headerlink" title="2.1 基本信息"></a>2.1 基本信息</h3><ul><li><p>2021年，ICLR会议论文</p></li><li><p>源码：<a href="https://github.com/ShuoYang-1998/Few_Shot_Distribution_Calibration">Few_Shot_Distribution_Calibration</a></p></li><li><p>提出从语义相似的基类(s)迁移统计数据来校准这些少数样本类的分布，接着<strong>依据新分布的均值和方差随机采样一定数量的样本</strong>，对novel classes的n_way k_shot任务的支持集进行补充，补充后的support set aug输入分类器fit。</p></li></ul><h3 id="2-2-方法记录"><a href="#2-2-方法记录" class="headerlink" title="2.2 方法记录"></a>2.2 方法记录</h3><p>前提假设：假设特征嵌入的每个维度都服从<strong>高斯分布</strong>。</p><p>这篇代码<strong>基类</strong>是有充足样本的类，用别人的SOTA分类模型，取倒数第二层作为特征提取器，用于提取基类和novel类的特征嵌入。接着计算每个基类特征层面的均值向量和协方差。</p><p>测试类(novel classes)用的是轮次训练的方式，例如：</p>    <div class="fold">      <div class="fold-title fold-info collapsed" data-toggle="collapse" href="#collapse-8f7bdefe" role="button" aria-expanded="false" aria-controls="collapse-8f7bdefe">        <div class="fold-arrow">▶</div>FSLTask参数      </div>      <div class="fold-collapse collapse" id="collapse-8f7bdefe">        <div class="fold-content">          <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs PYTHON"><span class="hljs-comment"># ---- data loading</span><br>    dataset = <span class="hljs-string">&#x27;miniImagenet&#x27;</span><br>    n_shot = <span class="hljs-number">1</span><br>    n_ways = <span class="hljs-number">5</span><br>    n_queries = <span class="hljs-number">15</span><br>    n_runs = <span class="hljs-number">10000</span><br>    n_lsamples = n_ways * n_shot<br>    n_usamples = n_ways * n_queries<br>    n_samples = n_lsamples + n_usamples<br></code></pre></td></tr></table></figure>        </div>      </div>    </div><p>每次从novel classes中抽5个类，每个类有1个support sample，15个query sample。每一次run，对抽取的5个support_data(先转成特征嵌入)分别做分布校准，然后生成若干个数的特征向量作为support集的<strong>补充</strong>，一起输入分类器fit，在query samples上测试，计算acc，最后取10000次run的平均acc。</p><p>分布校准core代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs PYTHON"><span class="hljs-keyword">def</span> <span class="hljs-title function_">distribution_calibration</span>(<span class="hljs-params">query, base_means, base_cov, k,alpha=<span class="hljs-number">0.21</span></span>):<br>    dist = []  <span class="hljs-comment"># 计算support sample(变量名叫query)和mean之间的欧式距离</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(base_means)):<br>        dist.append(np.linalg.norm(query-base_means[i]))<br>    index = np.argpartition(dist, k)[:k]  <span class="hljs-comment"># 从数组 dist 中找到前𝑘小的元素的索引</span><br>    mean = np.concatenate([np.array(base_means)[index], query[np.newaxis, :]])<br>    calibrated_mean = np.mean(mean, axis=<span class="hljs-number">0</span>)<br>    calibrated_cov = np.mean(np.array(base_cov)[index], axis=<span class="hljs-number">0</span>)+alpha<br><br>    <span class="hljs-keyword">return</span> calibrated_mean, calibrated_cov<br></code></pre></td></tr></table></figure><div class="note note-primary">            <ol><li>这篇文章在<strong>细粒度数据集CUB</strong>上也应用了DC方法，有超过200种不同的鸟类图片。每个人的脑电虽然说各自有判别性特征，其实还是很相似的细粒度数据，这个方法通过迁移<span class="label label-secondary">k</span>个距离最小的基类的分布，来直接增加支持集的样本，进而提高分类性能。</li><li>要调的超参数多，采样个数、k个基类、离散程度a。</li><li>在脑电上是否有效呢？特征向量都是经过标准化&#x2F;幂变换的。</li></ol>          </div><h2 id="3-Semantic-Based-Implicit-Feature-Transform-for-Few-Shot-Classification"><a href="#3-Semantic-Based-Implicit-Feature-Transform-for-Few-Shot-Classification" class="headerlink" title="3.Semantic-Based Implicit Feature Transform for Few-Shot Classification"></a>3.Semantic-Based Implicit Feature Transform for Few-Shot Classification</h2><h3 id="3-1-基本信息"><a href="#3-1-基本信息" class="headerlink" title="3.1 基本信息"></a>3.1 基本信息</h3><ul><li><p>2024年，International Journal of Computer Vision</p></li><li><p>源码：<a href="https://github.com/pmhDL/SIFT.git">SIFT</a></p></li><li><p>借鉴的是前面分布矫正的论文，同样是通过从基类补充特征向量到novel类上面去，不同的是图像类别标签（如cat、dog），<strong>本身具有语义信息</strong>，可以用不同的词向量来表达。然后通过语义嵌入的相似度来选择最近的基类，而不是前面通过统计特征。</p></li></ul><h3 id="3-2-方法记录"><a href="#3-2-方法记录" class="headerlink" title="3.2 方法记录"></a>3.2 方法记录</h3><ul><li><p>脑电的类别标签纯粹是<span class="label label-secondary">'Person A'</span>、<span class="label label-secondary">'Person B'</span>的形式，并不具有语义信息，所以如果要用，应该采用Free Lunch的方法<strong>做跨时段的分布矫正</strong>。</p></li><li><p>提出了一种原型矫正的方法。适用于transductive setting，即查询样本全部一次给出的情况。通过K-means把查询样本分簇为N类，接着建立路径规划问题的数学模型，为这N个类别确定互相不重复的标签，与初始的每个类原型做平均。</p></li></ul>    <div class="fold">      <div class="fold-title fold-info collapsed" data-toggle="collapse" href="#collapse-6ae3e036" role="button" aria-expanded="false" aria-controls="collapse-6ae3e036">        <div class="fold-arrow">▶</div>原型矫正实现      </div>      <div class="fold-collapse collapse" id="collapse-6ae3e036">        <div class="fold-content">          <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs PYTHON"><span class="hljs-keyword">def</span> <span class="hljs-title function_">updateproto</span>(<span class="hljs-params">Xs, ys, cls_center, way</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;更新原型 (prototype)，并与聚类中心 (cluster center) 结合&quot;&quot;&quot;</span><br>    <br>    <span class="hljs-comment"># 调用 np_proto 计算每个类别的原型</span><br>    proto = np_proto(Xs, ys, way)  <br>    <br>    <span class="hljs-comment"># 计算每个类别原型与所有聚类中心之间的平方欧式距离</span><br>    <span class="hljs-comment"># 使用广播机制，扩展 proto 和 cls_center 的维度后相减</span><br>    dist = ((proto[:, np.newaxis, :] - cls_center[np.newaxis, :, :]) ** <span class="hljs-number">2</span>).<span class="hljs-built_in">sum</span>(<span class="hljs-number">2</span>)  <br>    <br>    <span class="hljs-comment"># 找到每个类别原型距离最近的聚类中心索引</span><br>    <span class="hljs-built_in">id</span> = dist.argmin(<span class="hljs-number">1</span>)  <br>    feat_proto = np.zeros((way, Xs.shape[<span class="hljs-number">1</span>]))  <br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(way):<br>        <span class="hljs-comment"># 将当前类别的原型和其最近的聚类中心取平均值</span><br>        feat_proto[i] = (cls_center[<span class="hljs-built_in">id</span>[i]] + proto[i]) / <span class="hljs-number">2</span>  <br>    <br>    <span class="hljs-keyword">return</span> feat_proto  <br></code></pre></td></tr></table></figure>        </div>      </div>    </div><h2 id="4-Matching-Feature-Sets-for-Few-Shot-Image-Classification"><a href="#4-Matching-Feature-Sets-for-Few-Shot-Image-Classification" class="headerlink" title="4.Matching Feature Sets for Few-Shot Image Classification"></a>4.Matching Feature Sets for Few-Shot Image Classification</h2><h3 id="4-1-基本信息"><a href="#4-1-基本信息" class="headerlink" title="4.1 基本信息"></a>4.1 基本信息</h3><ul><li><p>2022年，CVPR会议论文</p></li><li><p>源码：<a href="https://lvsn.github.io/SetFeat/">SetFeat-fs</a></p></li><li><p>以Conv4-64骨干网络为例，提出把一张图像输入进去，经过一个Block，就输出一个经<strong>自注意力机制mapper</strong>计算后的特征向量$h_m$，这样处理从一张图片中提取出一组m(4)个特征向量。距离度量用的是负余弦相似度，实际上和原型网络的距离度量很相似，区分在于有多个特征向量，它这里<strong>统一了向量的shape</strong>，相当于在同一个特征空间内，按mapper聚合多个类中心。</p></li></ul><h3 id="4-2-方法记录"><a href="#4-2-方法记录" class="headerlink" title="4.2 方法记录"></a>4.2 方法记录</h3><ul><li><p>如何利用多个不同层提取出的特征向量，核心如下公式所示。<br><img src="https://s2.loli.net/2025/01/09/NMQJKOybilHTAGV.png" alt="image.png"></p></li><li><p><strong>训练流程：</strong>两个阶段，<span class="label label-primary">第一阶段</span>就是正常带FC层的分类器，并且本文是在每个Block后面都接一个FC层，分别训练到这个Block为止的网络，<span class="label label-primary">第二阶段</span>，舍去FC层，应用轮次训练在基类上模拟FSL Task，通过公式6的负对数概率计算loss，反向传播微调编码器的参数。微调结束，最终在novel类上面推理，之前的Free Lunch也是一样的流程。<br><img src="https://s2.loli.net/2025/01/09/eOXMZnHkxYScaRP.png" alt="image.png"></p></li></ul><div class="note note-secondary">            <ol><li>总结一下，它提高原型网络分类精度的手段就是，同一个特征space，一个类别聚合m个类中心。</li><li>感觉第一阶段训练就相当于独立地训练了4个encoder，但是又不独立，前面的参数是要重复使用的，具体要看代码train部分。</li></ol>          </div><div class="note note-info">            <ol><li>这种在预训练阶段，训练一个分类器的做法和我之前看的<strong>有监督对比学习</strong>训练编码器的方式不一样，我的想法是也许可以借用这个metric，然后用SCL的做法分别独立训练出3个encoder。之后可以有两种metric方法，一种是和本文做法一样，投射到同一个特征空间；另一种映射到不同特征空间分别做相似度计算，再求和，作为新的metric。</li><li>还有一个想法是SCL的<strong>encoder很关键，</strong>决定了特征向量的质量，借鉴GoogleNet的多尺度卷积方法，预训练出一个encoder，也可以试一下。</li></ol>          </div><h3 id="4-3-参考博客"><a href="#4-3-参考博客" class="headerlink" title="4.3 参考博客"></a>4.3 参考博客</h3><ul><li><p><a href="https://blog.csdn.net/weixin_43499457/article/details/124595010?ops_request_misc=%257B%2522request%255Fid%2522%253A%25223c42cb44ac6d1d4ee40531845d5a092b%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=3c42cb44ac6d1d4ee40531845d5a092b&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-1-124595010-null-null.142%5Ev101%5Epc_search_result_base8&utm_term=%E7%9B%91%E7%9D%A3%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0&spm=1018.2226.3001.4187">有监督对比学习在分类任务中的应用 Supervised Contrastive Learning</a></p></li><li><p><a href="https://blog.csdn.net/c___c18/article/details/144056112?ops_request_misc=&request_id=&biz_id=102&utm_term=%E7%9B%91%E7%9D%A3%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-1-144056112.142%5Ev101%5Epc_search_result_base8&spm=1018.2226.3001.4187">监督对比学习代码实现与分析（Supervised Contrastive Learning in NLP）</a></p></li><li><p><a href="https://blog.csdn.net/Bluebro/article/details/130035707">主干网络backbone讲解—— Conv4与Resnet12</a></p></li></ul><h2 id="5-BSNet-Bi-Similarity-Network-for-Few-shot-Fine-grained-Image-Classification"><a href="#5-BSNet-Bi-Similarity-Network-for-Few-shot-Fine-grained-Image-Classification" class="headerlink" title="5.BSNet: Bi-Similarity Network for Few-shot  Fine-grained Image Classification"></a>5.BSNet: Bi-Similarity Network for Few-shot  Fine-grained Image Classification</h2><h3 id="5-1-基本信息"><a href="#5-1-基本信息" class="headerlink" title="5.1 基本信息"></a>5.1 基本信息</h3><ul><li><p>2020年，‌IEEE Transactions on Image Processing（TIP）期刊论文</p></li><li><p>源码：<a href="https://github.com/PRIS-CV/BSNet">BSNet</a></p></li><li><p>代码格式简洁，是基于度量学习的图像细粒度分类，提出同时使用余弦相似度和欧式距离两种loss，即在ProtoNet上添加一个余弦相似度的loss，分类精度在图像分类上有所提高。</p></li></ul><div class="note note-light">            <p>最后调优模型的时候可以试一下，有空看代码。</p>          </div> <h2 id="6-Supervised-Contrastive-Learning"><a href="#6-Supervised-Contrastive-Learning" class="headerlink" title="6.Supervised Contrastive Learning"></a>6.Supervised Contrastive Learning</h2><h3 id="6-1-基本信息"><a href="#6-1-基本信息" class="headerlink" title="6.1 基本信息"></a>6.1 基本信息</h3><ul><li><p>2020年，NeurIPS</p></li><li><p>源码：<a href="https://github.com/HobbitLong/SupContrast">SupContrast</a></p></li><li><p>SCL集合了传统度量学习领域的<span class="label label-secondary">Triplet loss</span>、<span class="label label-secondary">N-pair loss</span>两者的特点，提出对一个Anchor除了<strong>考虑多个负样例之外，也同时考虑多个正样例</strong>，设计的损失函数避开了需要显式地调参以挖掘半难样本的需求。</p></li></ul><h3 id="6-2-方法记录"><a href="#6-2-方法记录" class="headerlink" title="6.2 方法记录"></a>6.2 方法记录</h3><ul><li><p>在一个batch中，每一个样本$x$都经过一个数据增强模块$Aug(·)$生成两个随机增强$\bar{x} &#x3D; Aug(x)$，两个增强后的样本标签一样，属于同一个类别。接下来<strong>如何保证随机生成的一个batch有多个样本标签相同呢</strong>，除了数据增强的方式生成正样本对之外，相对于类别个数大小C，batch的<code>batch_size = N</code>要远大于C，这样平均N&#x2F;C，就必定会采样到同一个类的多个样本。</p></li><li><p>代码与对应公式的解释：<a href="https://zhuanlan.zhihu.com/p/670579496">监督对比学习SupConLoss代码学习笔记</a></p></li></ul>    <div class="fold">      <div class="fold-title fold-info collapsed" data-toggle="collapse" href="#collapse-9c51fa20" role="button" aria-expanded="false" aria-controls="collapse-9c51fa20">        <div class="fold-arrow">▶</div>SupContrastLoss      </div>      <div class="fold-collapse collapse" id="collapse-9c51fa20">        <div class="fold-content">          <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><code class="hljs PYTHON"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Author: Yonglong Tian (yonglong@mit.edu)</span><br><span class="hljs-string">Date: May 07, 2020</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">from</span> __future__ <span class="hljs-keyword">import</span> print_function<br><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SupConLoss</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.</span><br><span class="hljs-string">    It also supports the unsupervised contrastive loss in SimCLR&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, temperature=<span class="hljs-number">0.07</span>, contrast_mode=<span class="hljs-string">&#x27;all&#x27;</span>,</span><br><span class="hljs-params">                 base_temperature=<span class="hljs-number">0.07</span></span>):<br>        <span class="hljs-built_in">super</span>(SupConLoss, self).__init__()<br>        self.temperature = temperature<br>        self.contrast_mode = contrast_mode<br>        self.base_temperature = base_temperature<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, features, labels=<span class="hljs-literal">None</span>, mask=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Compute loss for model. If both `labels` and `mask` are None,</span><br><span class="hljs-string">        it degenerates to SimCLR unsupervised loss:</span><br><span class="hljs-string">        https://arxiv.org/pdf/2002.05709.pdf</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            features: hidden vector of shape [bsz, n_views, ...].</span><br><span class="hljs-string">            labels: ground truth of shape [bsz].</span><br><span class="hljs-string">            mask: contrastive mask of shape [bsz, bsz], mask_&#123;i,j&#125;=1 if sample j</span><br><span class="hljs-string">                has the same class as sample i. Can be asymmetric.</span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            A loss scalar.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        device = (torch.device(<span class="hljs-string">&#x27;cuda&#x27;</span>)<br>                  <span class="hljs-keyword">if</span> features.is_cuda<br>                  <span class="hljs-keyword">else</span> torch.device(<span class="hljs-string">&#x27;cpu&#x27;</span>))<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(features.shape) &lt; <span class="hljs-number">3</span>:<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;`features` needs to be [bsz, n_views, ...],&#x27;</span><br>                             <span class="hljs-string">&#x27;at least 3 dimensions are required&#x27;</span>)<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(features.shape) &gt; <span class="hljs-number">3</span>:<br>            features = features.view(features.shape[<span class="hljs-number">0</span>], features.shape[<span class="hljs-number">1</span>], -<span class="hljs-number">1</span>)<br><br>        batch_size = features.shape[<span class="hljs-number">0</span>]  <span class="hljs-comment"># batch_size = N</span><br>        <span class="hljs-keyword">if</span> labels <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Cannot define both `labels` and `mask`&#x27;</span>)<br>        <span class="hljs-keyword">elif</span> labels <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> mask <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            mask = torch.eye(batch_size, dtype=torch.float32).to(device)<br>        <span class="hljs-keyword">elif</span> labels <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            labels = labels.contiguous().view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>            <span class="hljs-keyword">if</span> labels.shape[<span class="hljs-number">0</span>] != batch_size:<br>                <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Num of labels does not match num of features&#x27;</span>)<br>            mask = torch.eq(labels, labels.T).<span class="hljs-built_in">float</span>().to(device)<br>        <span class="hljs-keyword">else</span>:<br>            mask = mask.<span class="hljs-built_in">float</span>().to(device)<br><br>        contrast_count = features.shape[<span class="hljs-number">1</span>]  <span class="hljs-comment"># 视图个数</span><br>        contrast_feature = torch.cat(torch.unbind(features, dim=<span class="hljs-number">1</span>), dim=<span class="hljs-number">0</span>)  <span class="hljs-comment"># 消除视图维度，按batch拼接，得到[N*n_views, feature_dim]</span><br>        <span class="hljs-keyword">if</span> self.contrast_mode == <span class="hljs-string">&#x27;one&#x27;</span>:<br>            anchor_feature = features[:, <span class="hljs-number">0</span>]  <span class="hljs-comment"># 只取第一个视图作为anchor</span><br>            anchor_count = <span class="hljs-number">1</span><br>        <span class="hljs-keyword">elif</span> self.contrast_mode == <span class="hljs-string">&#x27;all&#x27;</span>:<br>            anchor_feature = contrast_feature  <span class="hljs-comment"># 所有视图作为anchor</span><br>            anchor_count = contrast_count<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Unknown mode: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.contrast_mode))<br><br>        <span class="hljs-comment"># compute logits</span><br>        anchor_dot_contrast = torch.div(<br>            torch.matmul(anchor_feature, contrast_feature.T),  <span class="hljs-comment"># 1.单个 [N, ft_dim] * [ft_dim, N*n_views] = [N, N*n_views]  2.所有 [N*n_views, ft_dim] * [ft_dim, N*n_views] = [N*n_views, N*n_views]</span><br>            self.temperature)<br>        <span class="hljs-comment"># for numerical stability</span><br>        logits_max, _ = torch.<span class="hljs-built_in">max</span>(anchor_dot_contrast, dim=<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>        logits = anchor_dot_contrast - logits_max.detach()<br><br>        <span class="hljs-comment"># tile mask</span><br>        mask = mask.repeat(anchor_count, contrast_count)<br>        <span class="hljs-comment"># mask-out self-contrast cases</span><br>        logits_mask = torch.scatter(<br>            torch.ones_like(mask),<br>            <span class="hljs-number">1</span>,<br>            torch.arange(batch_size * anchor_count).view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>).to(device),<br>            <span class="hljs-number">0</span><br>        )<br>        mask = mask * logits_mask<br><br>        <span class="hljs-comment"># compute log_prob</span><br>        exp_logits = torch.exp(logits) * logits_mask  <span class="hljs-comment"># [N*n_views, N*n_views]</span><br>        log_prob = logits - torch.log(exp_logits.<span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>))  <span class="hljs-comment"># [N*n_views, N*n_views]</span><br><br>        <span class="hljs-comment"># compute mean of log-likelihood over positive</span><br>        <span class="hljs-comment"># modified to handle edge cases when there is no positive pair</span><br>        <span class="hljs-comment"># for an anchor point.</span><br>        <span class="hljs-comment"># Edge case e.g.:-</span><br>        <span class="hljs-comment"># features of shape: [4,1,...]</span><br>        <span class="hljs-comment"># labels:            [0,1,1,2]</span><br>        <span class="hljs-comment"># loss before mean:  [nan, ..., ..., nan]</span><br>        mask_pos_pairs = mask.<span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span>)<br>        mask_pos_pairs = torch.where(mask_pos_pairs &lt; <span class="hljs-number">1e-6</span>, <span class="hljs-number">1</span>, mask_pos_pairs)<br>        mean_log_prob_pos = (mask * log_prob).<span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span>) / mask_pos_pairs  <span class="hljs-comment"># [N*n_views]</span><br><br>        <span class="hljs-comment"># loss</span><br>        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos <span class="hljs-comment"># [N*n_views]</span><br>        loss = loss.view(anchor_count, batch_size).mean()  <span class="hljs-comment"># 标量</span><br><br>        <span class="hljs-keyword">return</span> loss<br></code></pre></td></tr></table></figure>        </div>      </div>    </div><div class="note note-info">            <p>仿照这篇论文的三个核心模块：<strong>数据增强模块</strong>、编码网络、映射网络，计划对脑电原始数据做Channel Reflection数据增强，以及一个其他数据增强操作，和本文的监督对比loss匹配，Backbone也需要找一个替换。</p>          </div><h2 id="7-Channel-reflection-Knowledge-driven-data-augmentation-for-EEG-based-brain–computer-interfaces"><a href="#7-Channel-reflection-Knowledge-driven-data-augmentation-for-EEG-based-brain–computer-interfaces" class="headerlink" title="7.Channel reflection: Knowledge-driven data augmentation for EEG-based brain–computer interfaces"></a>7.Channel reflection: Knowledge-driven data augmentation for EEG-based brain–computer interfaces</h2><h3 id="7-1-基本信息"><a href="#7-1-基本信息" class="headerlink" title="7.1 基本信息"></a>7.1 基本信息</h3><ul><li><p>2024年，Neural Networks</p></li><li><p>源码：<a href="https://github.com/wzwvv/EEGAug">EEGAug</a></p></li><li><p>提出了一种无需超参数的<strong>通道交换(CR)数据增强</strong>方法，传统的数据增强如添加Noise、Scale、Frep都需要调超参数，并且十分鲁棒，在MI、SSVEP、ERP、癫痫检测4个实验范式下均适用，相比于Baseline(没有数据增强)，分类精度更好。注意经CR数据增强之后，训练数据翻倍。</p></li></ul><h3 id="7-2-方法记录"><a href="#7-2-方法记录" class="headerlink" title="7.2 方法记录"></a>7.2 方法记录</h3>    <div class="fold">      <div class="fold-title fold-info collapsed" data-toggle="collapse" href="#collapse-05eef982" role="button" aria-expanded="false" aria-controls="collapse-05eef982">        <div class="fold-arrow">▶</div>leftrightflipping_transform      </div>      <div class="fold-collapse collapse" id="collapse-05eef982">        <div class="fold-content">          <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs PYTHON"><span class="hljs-keyword">def</span> <span class="hljs-title function_">leftrightflipping_transform</span>(<span class="hljs-params">X, left_mat, right_mat</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Parameters</span><br><span class="hljs-string">    ----------</span><br><span class="hljs-string">    X: torch tensor of shape (num_samples, 1, num_channels, num_timesamples)</span><br><span class="hljs-string">    left_mat: numpy array of shape (a, ), where a is the number of left brain channels, in order</span><br><span class="hljs-string">    right_mat: numpy array of shape (b, ), where b is the number of right brain channels, in order</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns</span><br><span class="hljs-string">    -------</span><br><span class="hljs-string">    transformedX: transformed signal of torch tensor of shape (num_samples, num_channels, num_timesamples)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    num_samples, _, num_channels, num_timesamples = X.shape<br>    transformedX = torch.zeros((num_samples, <span class="hljs-number">1</span>, num_channels, num_timesamples))<br>    <span class="hljs-keyword">for</span> ch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_channels):<br>        <span class="hljs-keyword">if</span> ch <span class="hljs-keyword">in</span> left_mat:<br>            ind = left_mat.index(ch)<br>            transformedX[:, <span class="hljs-number">0</span>, ch, :] = X[:, <span class="hljs-number">0</span>, right_mat[ind], :]<br>        <span class="hljs-keyword">elif</span> ch <span class="hljs-keyword">in</span> right_mat:<br>            ind = right_mat.index(ch)<br>            transformedX[:, <span class="hljs-number">0</span>, ch, :] = X[:, <span class="hljs-number">0</span>, left_mat[ind], :]<br>        <span class="hljs-keyword">else</span>:<br>            transformedX[:, <span class="hljs-number">0</span>, ch, :] = X[:, <span class="hljs-number">0</span>, ch, :]<br><br>    <span class="hljs-keyword">return</span> transformedX<br></code></pre></td></tr></table></figure>        </div>      </div>    </div><ul><li>即根据脑电极通道的索引，位于中线的电极不变，左脑和右脑对称分布的电极做一个交换，类似于图像的翻转操作。除左右手MI想象要调换标签之外，ERP不需要换标签。调用方法如下：</li></ul>    <div class="fold">      <div class="fold-title fold-info collapsed" data-toggle="collapse" href="#collapse-90469042" role="button" aria-expanded="false" aria-controls="collapse-90469042">        <div class="fold-arrow">▶</div>调用example      </div>      <div class="fold-collapse collapse" id="collapse-90469042">        <div class="fold-content">          <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs PYTHON"><span class="hljs-keyword">if</span> dataset == <span class="hljs-string">&#x27;BNCI2014001&#x27;</span>:<br>    left_mat = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">13</span>, <span class="hljs-number">14</span>, <span class="hljs-number">18</span>]<br>    right_mat = [<span class="hljs-number">5</span>, <span class="hljs-number">4</span>, <span class="hljs-number">12</span>, <span class="hljs-number">11</span>, <span class="hljs-number">10</span>, <span class="hljs-number">17</span>, <span class="hljs-number">16</span>, <span class="hljs-number">20</span>]<br>    aug_train_x = leftrightflipping_transform(<br>        torch.from_numpy(train_x).to(torch.float32).reshape(train_x.shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>, ch_num, -<span class="hljs-number">1</span>),<br>        left_mat, right_mat).numpy().reshape(train_x.shape[<span class="hljs-number">0</span>], ch_num, -<span class="hljs-number">1</span>)<br>    aug_train_y = <span class="hljs-number">1</span> - train_y  <span class="hljs-comment"># 二分类标签0-1对调</span><br></code></pre></td></tr></table></figure>        </div>      </div>    </div><ul><li><a href="https://roses.blog.csdn.net/article/details/141632317?spm=1001.2101.3001.6650.3&utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-3-141632317-blog-128355505.235%5Ev43%5Epc_blog_bottom_relevance_base4&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-3-141632317-blog-128355505.235%5Ev43%5Epc_blog_bottom_relevance_base4&utm_relevant_index=6">​华中科技大学伍冬睿团队提出知识数据融合的通道交换脑电数据增强方法</a></li></ul><h2 id="8-Charting-the-Right-Manifold-Manifold-Mixup-for-Few-shot-Learning"><a href="#8-Charting-the-Right-Manifold-Manifold-Mixup-for-Few-shot-Learning" class="headerlink" title="8.Charting the Right Manifold: Manifold Mixup for Few-shot Learning"></a>8.Charting the Right Manifold: Manifold Mixup for Few-shot Learning</h2><h3 id="8-1-基本信息"><a href="#8-1-基本信息" class="headerlink" title="8.1 基本信息"></a>8.1 基本信息</h3><ul><li><p>2020年，WACV</p></li><li><p>源码：<a href="https://github.com/nupurkmr9/S2M2_fewshot">S2M2_fewshot</a></p></li><li><p>提出…</p></li></ul></font>]]></content>
    
    
    <categories>
      
      <category>metric-learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>paper</tag>
      
      <tag>metric-learning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PowerPoint修炼</title>
    <link href="/cn//"/>
    <url>/cn//</url>
    
    <content type="html"><![CDATA[<font size=3><h1 id="1-PPT整体展示图表之后局部高亮"><a href="#1-PPT整体展示图表之后局部高亮" class="headerlink" title="1.PPT整体展示图表之后局部高亮"></a>1.PPT整体展示图表之后局部高亮</h1><p><a href="https://www.bilibili.com/video/BV13r4y1c7r1/?vd_source=a9e78e47d3e6d67d875c0260caff8550">步骤教学</a></p><p><img src="https://s2.loli.net/2024/12/07/KUh4etcvqSk2BDd.gif" alt="ppt.gif"></p><h1 id="2-Others"><a href="#2-Others" class="headerlink" title="2.Others"></a>2.Others</h1><ul><li><a href="https://www.freeconvert.com/zh/convert/video-to-gif">视频转GIF工具</a></li><li><a href="https://blog.csdn.net/weixin_38314865/article/details/104440652">PPT立方体形状变薄</a></li></ul></font>]]></content>
    
    
    
    <tags>
      
      <tag>PPT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>评估指标记录</title>
    <link href="/cn//"/>
    <url>/cn//</url>
    
    <content type="html"><![CDATA[<font size=3><p>电子笔记</p><span id="more"></span><h1 id="1-AUC-are-under-curve"><a href="#1-AUC-are-under-curve" class="headerlink" title="1.AUC(are under curve)"></a>1.AUC(are under curve)</h1><h2 id="1-1什么是AUC"><a href="#1-1什么是AUC" class="headerlink" title="1.1什么是AUC"></a>1.1什么是AUC</h2><p>用于评估分类器的分类效果，对于真实类别为1的样本，分类器预测为1的概率(即TPrate)要大于真实类别为0而预测类别为1的概率(即FPrate)，即AUC&gt;0.5。</p><h2 id="1-2如何计算AUC"><a href="#1-2如何计算AUC" class="headerlink" title="1.2如何计算AUC"></a>1.2如何计算AUC</h2><p>在有M个正样本,N个负样本的数据集里。一共有M*N对样本（一对样本即一个正样本与一个负样本）。统计这M*N对样本里，正样本的预测概率大于负样本的预测概率的个数。<br><img src="https://s2.loli.net/2024/12/09/lj6xh4XCm7oqgDu.png" alt="image.png"></p><h2 id="1-3接口参数含义"><a href="#1-3接口参数含义" class="headerlink" title="1.3接口参数含义"></a>1.3接口参数含义</h2><p><code>roc_auc_score(test_labels, probs, multi_class=&#39;ovr&#39;)</code></p><div class="note note-info">            <ol><li>test_labels: 实际的类别标签，shape是 [N,]</li><li>probs: 每个样本在各个类别上的概率，shape是[N, C]</li><li>指定计算多类别问题的 AUC (<strong>One-vs-Rest 策略</strong>)</li></ol>          </div><h1 id="2-AccuracyCalculator使用"><a href="#2-AccuracyCalculator使用" class="headerlink" title="2.AccuracyCalculator使用"></a>2.AccuracyCalculator使用</h1><p>在<strong>pytorch-metric-learning</strong>中，AccuracyCalculator主要用于计算准确率（如 top-k 准确率）。通过嵌入向量之间的相似度来推测每个样本的类别，并与真实标签进行比较，相同则预测正确。这种方法是基于<strong>最近邻</strong>来判断类别，预测准确率的。</p><p>使用方法：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">from</span> pytorch_metric_learning <span class="hljs-keyword">import</span> losses<br><span class="hljs-keyword">from</span> pytorch_metric_learning.utils.accuracy_calculator <span class="hljs-keyword">import</span> AccuracyCalculator<br><br><span class="hljs-comment"># 初始化 AccuracyCalculator: 参数k值，指定 top-k 精度</span><br>accuracy_calculator = AccuracyCalculator(k=<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># 计算准确率</span><br>accuracy = accuracy_calculator.calculate(<br>    embeddings=embeddings,  <span class="hljs-comment"># 模型输出的嵌入向量 [batch_size, embedding_dim]</span><br>    labels=labels,          <span class="hljs-comment"># 标签 [batch_size]</span><br>)<br></code></pre></td></tr></table></figure><h1 id="3-资料"><a href="#3-资料" class="headerlink" title="3.资料"></a>3.资料</h1><ul><li><a href="https://www.zhihu.com/question/39840928?from=profile_question_card">AUC如何理解？</a></li><li><a href="https://blog.csdn.net/qq_22238533/article/details/78666436">AUC的计算方法</a></li></ul></font>]]></content>
    
    
    <categories>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>pytorch</tag>
      
      <tag>sklearn</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ear-EEG数据集调研</title>
    <link href="/cn//"/>
    <url>/cn//</url>
    
    <content type="html"><![CDATA[<font size=3><p>有关<strong>耳脑电</strong>的概述、4个应用类别的可行性、公开数据集的材料调研。</p><span id="more"></span><h1 id="1-概述"><a href="#1-概述" class="headerlink" title="1.概述"></a>1.概述</h1><p>和全头皮脑电类似，ear-EEG同样是通过在皮肤上放置电极记录微小的电压变化，以测量动态的大脑活动。电极传感器可以放置在外耳的不同位置（耳廓、耳甲、耳道），检测设备材料种类很多，测量方式也分干&#x2F;湿电极，但是<strong>与头皮脑电相比，耳脑电的电极部署数量少，且信号幅度(signal amplitude)明显低于头皮脑电</strong>10-20dB。</p><p>耳脑电靠近<strong>颞叶(Temporal Lobe)区域</strong>，这个区域和听觉、语言处理、记忆和情感都相关，在事件相关电位(ERPs)、脑机接口(BCI)、睡眠、癫痫4个领域都已有研究来证明耳脑电信号的可行性。</p><h1 id="2-应用领域"><a href="#2-应用领域" class="headerlink" title="2.应用领域"></a>2.应用领域</h1><h2 id="2-1-事件相关电位"><a href="#2-1-事件相关电位" class="headerlink" title="2.1 事件相关电位"></a>2.1 事件相关电位</h2><p>ERP是测量的大脑对视觉、声音或触觉的短暂刺激的反应。耳脑电被证明可可靠测量几个熟知的ERPs，耳脑电实现的EEG诱发范式大多数与听觉刺激有关，包括<strong>听觉诱发电位</strong>(auditory-evoked potentials，AEPs)和<strong>听觉稳态电位</strong>(multiple auditory-steady responses, ASSR)，以及<strong>SSVEP</strong>。</p><h2 id="2-2-脑机接口"><a href="#2-2-脑机接口" class="headerlink" title="2.2 脑机接口"></a>2.2 脑机接口</h2><p>BCIs是用于解码电生理信号去控制一个外部设备的技术。综述中介绍了多篇paper采集耳脑电然后用于注意力状态鉴别、驾驶员嗜睡分类。</p><h2 id="2-3-睡眠"><a href="#2-3-睡眠" class="headerlink" title="2.3 睡眠"></a>2.3 睡眠</h2><p>耳脑电适合于长期的监测睡眠质量，耳内传感器获取的数据可以被用于睡眠分期的评估，但也提到耳脑电的<strong>amplitude</strong>比标准EEG低。</p><h2 id="2-4-癫痫"><a href="#2-4-癫痫" class="headerlink" title="2.4 癫痫"></a>2.4 癫痫</h2><p>适合长期监测，指出耳脑电适合记录颞叶癫痫信号，并用于癫痫评估。</p><h1 id="3-公开数据集"><a href="#3-公开数据集" class="headerlink" title="3.公开数据集"></a>3.公开数据集</h1><h2 id="3-1-睡眠监测"><a href="#3-1-睡眠监测" class="headerlink" title="3.1 睡眠监测"></a>3.1 睡眠监测</h2><ul><li><a href="https://openneuro.org/datasets/ds004348/versions/1.0.5">Ear-EEG Sleep Monitoring 2017 (EESM17)</a></li></ul><p>睡眠监测数据集，包含9名受试者的夜间脑电图记录。这些recording包括<strong>脑电图、眼电图和下巴肌电图，以及 14 个耳部脑电图电极</strong>。</p><p>label分为两类，1:wake; 2:sleep</p><h2 id="3-2-运动想象"><a href="#3-2-运动想象" class="headerlink" title="3.2 运动想象"></a>3.2 运动想象</h2><ul><li><a href="https://ieee-dataport.org/open-access/ear-eeg-recording-brain-computer-interface-motor-task">Ear-EEG Recording for Brain Computer Interface of Motor Task</a></li></ul><p>左&#x2F;右手抓握运动想象数据集，包括6个受试者。耳部脑电图与头皮脑电图同时记录。8个耳电极放置在前耳道和后耳道（标记为 xF、xB）<br>以及耳甲的两个上部和下部位置（标记为 xOU 和 xOD）。所有耳部和头皮电极均以头皮REF电极为参考。<br>头皮GRD电极用作接地参考。以1000 Hz采样信号，然后用0.5 Hz至100 Hz之间的带通滤波器和陷波滤波器进行滤波。</p><h2 id="3-3-SSVEP"><a href="#3-3-SSVEP" class="headerlink" title="3.3 SSVEP"></a>3.3 SSVEP</h2><ul><li><a href="http://deepbci.korea.ac.kr/opensource/opendb/">EEG Dataset for SSVEP using Ear-EEG and Scalp-EEG</a></li></ul><p>数据集是韩国的一个网站，需要申请：<br><img src="https://s2.loli.net/2024/11/28/XMAbZFmxgo6rzfL.png" alt="image.png"></p><ul><li><a href="https://ieeexplore.ieee.org/abstract/document/8758838">论文_包含实验范式</a></li></ul><h3 id="3-3-1-论文摘要"><a href="#3-3-1-论文摘要" class="headerlink" title="3.3.1 论文摘要"></a>3.3.1 论文摘要</h3><p>耳-脑电对电极位置有自然的限制(例如，限制在耳内或耳周围)，无法充分获取信息丰富的大脑信号。在特定的BCI范式中，不利用<strong>耳周围颞叶的脑信号</strong>，实现可靠的耳-脑电性能是困难的。<br>例如，<strong>稳态视觉诱发电位(SSVEPs)主要产生于枕区，在耳-脑电中具有明显的衰减和扭曲幅度。</strong>因此，保持高水平的解码精度对于基于耳-脑电的SSVEP BCI是具有挑战性和必要的。<br>本文首先研究了在SSVEP范式下，利用枕区估计的目标脑电信号，采用线性和非线性回归方法来提高耳-脑电解码精度。…</p><h3 id="3-3-2-实验范式"><a href="#3-3-2-实验范式" class="headerlink" title="3.3.2 实验范式"></a>3.3.2 实验范式</h3><p>实验一共3个session，前两个session同时记录头皮脑电和耳脑电，第3个session只记录耳脑电。刺激频率有3种，一次trial范式如图所示。<br><img src="https://s2.loli.net/2024/11/28/EtQj2Y6CcBbxryN.png" alt="image.png"></p><p>11个subjects，采样频率500hz，0.3-50 Hz的带通滤波器和60Hz陷波滤波器。</p><ul><li>session1 和 session2，每类刺激50个trial，一共150trial。</li><li>session3，只获取耳-脑电图信号，每个类20个trial，一共60个trial。</li><li>三个session是不在<strong>不同的天</strong>获得的。</li></ul><p><img src="https://s2.loli.net/2024/11/28/hEjdCFwfsqXQLW5.png" alt="image.png"></p><h1 id="4-参考资料"><a href="#4-参考资料" class="headerlink" title="4.参考资料"></a>4.参考资料</h1><ul><li><a href="https://en.wikipedia.org/wiki/Ear-EEG#History">Ear-EEG_wikipedia</a></li><li><a href="http://deepbci.korea.ac.kr/wp-content/uploads/2019/05/Discription.pdf">韩国ssvep数据集的范式说明</a></li><li>Ear-EEG Devices for the Assessment of Brain  Activity: A Review</li></ul></font>]]></content>
    
    
    
    <tags>
      
      <tag>ear-EEG</tag>
      
      <tag>database</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>面向对象实现文件下载系统</title>
    <link href="/cn//"/>
    <url>/cn//</url>
    
    <content type="html"><![CDATA[<p>Server端代码</p><span id="more"></span><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-comment">#!/usr/bin/python3</span><br><span class="hljs-comment"># @Time    : 2024/8/24 9:46</span><br><span class="hljs-comment"># @Author  : Proton</span><br><span class="hljs-comment"># @FileName: netdisk_server.py</span><br><span class="hljs-keyword">from</span> socket <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">import</span> struct<br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Server</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, ip, port</span>):<br>        self.s_listen:socket = <span class="hljs-literal">None</span><br>        self.ip = ip<br>        self.port = port<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">tcp_init</span>(<span class="hljs-params">self</span>):<br>        self.s_listen = socket(AF_INET, SOCK_STREAM)<br>        self.s_listen.bind((self.ip, self.port))<br>        self.s_listen.listen(<span class="hljs-number">128</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">task</span>(<span class="hljs-params">self</span>):<br>        new_client, client_addr = self.s_listen.accept()<br>        <span class="hljs-built_in">print</span>(client_addr)<br>        user = User(new_client)<br>        user.deal_command()<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">User</span>:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    每一个user对象对应一个客户端</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, new_client</span>):<br>        self.user: socket = new_client<br>        self.username = <span class="hljs-literal">None</span><br>        self.path = os.getcwd()  <span class="hljs-comment"># 存储连上的用户的路径</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train_send</span>(<span class="hljs-params">self, content_bytes</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        以开火车的形式发送，先发长度，再发内容</span><br><span class="hljs-string">        :param content:</span><br><span class="hljs-string">        :return:</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        content_len_bytes = struct.pack(<span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-built_in">len</span>(content_bytes))<br>        self.user.send(content_len_bytes+content_bytes)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train_recv</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        开火车接收，先接收文件名/内容长度，4B，再接收文件名/内容</span><br><span class="hljs-string">        :return:</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        train_head_bytes = self.user.recv(<span class="hljs-number">4</span>)<br>        train_head_len = struct.unpack(<span class="hljs-string">&#x27;I&#x27;</span>, train_head_bytes)<br>        <span class="hljs-comment"># 为什么不decode，因为不一定是文本，也可能是音乐or电影</span><br>        <span class="hljs-keyword">return</span> self.user.recv(train_head_len[<span class="hljs-number">0</span>])<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">deal_command</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>            <span class="hljs-comment"># 接收命令应该以开火车的形式收</span><br>            command = self.train_recv().decode(<span class="hljs-string">&#x27;utf8&#x27;</span>)<br>            <span class="hljs-keyword">if</span> command[:<span class="hljs-number">2</span>] == <span class="hljs-string">&#x27;ls&#x27;</span>:<br>                self.do_ls()<br>            <span class="hljs-keyword">elif</span> command[:<span class="hljs-number">2</span>] == <span class="hljs-string">&#x27;cd&#x27;</span>:<br>                self.do_cd(command)<br>            <span class="hljs-keyword">elif</span> command[:<span class="hljs-number">3</span>] == <span class="hljs-string">&#x27;pwd&#x27;</span>:<br>                self.do_pwd()<br>            <span class="hljs-keyword">elif</span> command[:<span class="hljs-number">2</span>] == <span class="hljs-string">&#x27;rm&#x27;</span>:<br>                self.do_rm(command)<br>            <span class="hljs-keyword">elif</span> command[:<span class="hljs-number">4</span>] == <span class="hljs-string">&#x27;puts&#x27;</span>:<br>                self.puts_file()<br>            <span class="hljs-keyword">elif</span> command[:<span class="hljs-number">4</span>] == <span class="hljs-string">&#x27;gets&#x27;</span>:<br>                self.gets_file(command)<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;wrong command.&#x27;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">do_ls</span>(<span class="hljs-params">self</span>):<br>        data = <span class="hljs-string">&#x27;&#x27;</span><br>        cur_list = os.listdir(<span class="hljs-string">&#x27;.&#x27;</span>)<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> cur_list:<br>            data += i + <span class="hljs-string">&#x27; &#x27;</span>*<span class="hljs-number">5</span> + <span class="hljs-built_in">str</span>(os.stat(i).st_size) + <span class="hljs-string">&#x27;\n&#x27;</span><br>        self.train_send(data.encode(<span class="hljs-string">&#x27;utf8&#x27;</span>))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">do_cd</span>(<span class="hljs-params">self, command</span>):<br>        path = command.split()[<span class="hljs-number">1</span>]<br>        os.chdir(path)<br>        self.path = os.getcwd()<br>        self.train_send(self.path.encode(<span class="hljs-string">&#x27;utf8&#x27;</span>))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">do_pwd</span>(<span class="hljs-params">self</span>):<br>        self.train_send(self.path.encode(<span class="hljs-string">&#x27;utf8&#x27;</span>))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">do_rm</span>(<span class="hljs-params">self, command</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        深度优先遍历，实现删除非空文件夹</span><br><span class="hljs-string">        :param command:</span><br><span class="hljs-string">        :return:</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        rm_target = command.split()[<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">if</span> os.path.isdir(rm_target) <span class="hljs-keyword">and</span> os.listdir(rm_target):  <span class="hljs-comment"># 若为非空目录</span><br>            file_list = os.listdir(rm_target)  <span class="hljs-comment"># 继续先删深处的文件</span><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> file_list:<br>                self.do_rm(command + <span class="hljs-string">&#x27;/&#x27;</span> + i)<br>            os.rmdir(rm_target)  <span class="hljs-comment"># 最后再删掉当前这个已经空了的目录</span><br>        <span class="hljs-keyword">elif</span> os.path.isdir(rm_target) <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> os.listdir(rm_target):  <span class="hljs-comment"># 空目录</span><br>            os.rmdir(rm_target)<br>        <span class="hljs-keyword">else</span>:  <span class="hljs-comment"># 普通文件</span><br>            os.remove(rm_target)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">puts_file</span>(<span class="hljs-params">self</span>):<br>        data = self.train_recv()<br>        file = <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;f[复件]&#x27;</span>, mode=<span class="hljs-string">&#x27;wb&#x27;</span>)<br>        file.write(data)<br>        file.close()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">gets_file</span>(<span class="hljs-params">self, command</span>):<br>        file_name = command.split()[<span class="hljs-number">1</span>]<br>        my_file = <span class="hljs-built_in">open</span>(file_name, mode=<span class="hljs-string">&#x27;rb&#x27;</span>)  <span class="hljs-comment"># 字节流形式打开二进制文件</span><br>        data = my_file.read()<br>        self.train_send(data)<br>        my_file.close()<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    s = Server(<span class="hljs-string">&#x27;&#x27;</span>, <span class="hljs-number">2000</span>)<br>    s.tcp_init()<br>    s.task()<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>epoll多路复用</title>
    <link href="/cn//"/>
    <url>/cn//</url>
    
    <content type="html"><![CDATA[<font size=3><p>为了实现及时聊天，即两方可以任意向对方发送连续的多条消息的功能，需要使用epoll。在内核中，socket对象缓冲区recv()、标准输入缓冲区input()都分配了一段内存，内存对应一个<strong>整型编号</strong>（数组下标），这个编号就是<strong>文件描述符file describer</strong>。</p><span id="more"></span><p>我们创建epoll对象，注册要监控的fd和事件类型，让epoll去监控哪几个缓冲区发生了指定事件，以列表的形式主动报告给用户进程。</p><h1 id="1-使用epoll编写即时聊天"><a href="#1-使用epoll编写即时聊天" class="headerlink" title="1.使用epoll编写即时聊天"></a>1.使用epoll编写即时聊天</h1><details><summary>服务器代码</summary><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">import</span> socket<br><span class="hljs-keyword">import</span> select<br><span class="hljs-keyword">import</span> sys<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tcp_server</span>():<br>    server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)<br>    s_addr = (<span class="hljs-string">&#x27;&#x27;</span>, <span class="hljs-number">2000</span>)<br>    server.bind(s_addr)<br>    server.listen(<span class="hljs-number">128</span>)  <span class="hljs-comment"># 被动监听，激活端口</span><br>    new_client, new_client_addr = server.accept()<br>    <span class="hljs-built_in">print</span>(new_client_addr)<br>    epoll = select.epoll()  <span class="hljs-comment"># 创建一个epoll对象</span><br>    <span class="hljs-comment"># 注册要监控的缓冲区，发生指定事件向用户进程汇报</span><br>    epoll.register(new_client.fileno(), select.EPOLLIN)<br>    epoll.register(sys.stdin.fileno(), select.EPOLLIN)<br>    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>        <span class="hljs-comment"># 一直等只会在这里卡住，谁的缓冲区有数据，就填写到eventslist，列表里变存元组 (fd, 事件)</span><br>        events_list = epoll.poll(-<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">for</span> fd,event <span class="hljs-keyword">in</span> events_list:<br>            <span class="hljs-keyword">if</span> fd == new_client.fileno():<br>                <span class="hljs-comment"># recv缓冲区有数据</span><br>                data = new_client.recv(<span class="hljs-number">100</span>).decode(<span class="hljs-string">&#x27;utf8&#x27;</span>)<br>                <span class="hljs-keyword">if</span> data:<br>                    <span class="hljs-built_in">print</span>(data)<br>                <span class="hljs-keyword">else</span>:<span class="hljs-comment"># 一旦对端断开，recv不会卡主，会返回空,内核会把client标记为一直可读</span><br>                    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;对方断开了&#x27;</span>)<br>                    <span class="hljs-keyword">return</span><br>            <span class="hljs-keyword">elif</span> fd == sys.stdin.fileno():<br>                <span class="hljs-comment"># input缓冲区有数据</span><br>                <span class="hljs-keyword">try</span>:  <span class="hljs-comment"># 按ctrl d让服务器断开</span><br>                    data = <span class="hljs-built_in">input</span>()<br>                <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>                    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;不想输入了，I want leave&#x27;</span>)<br>                    <span class="hljs-keyword">return</span><br>                new_client.send(data.encode(<span class="hljs-string">&#x27;utf8&#x27;</span>))<br>    server.close()<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    tcp_server()<br><br></code></pre></td></tr></table></figure></details><p><img src="https://s2.loli.net/2024/08/22/XPfIjBzTDE5MmiH.png" alt="即时聊天测试结果"></p><h1 id="2-使用epoll实现聊天室"><a href="#2-使用epoll实现聊天室" class="headerlink" title="2.使用epoll实现聊天室"></a>2.使用epoll实现聊天室</h1><div class="note note-info">            <p>实现多人聊天，</p><ol><li>新增客户端断开后可以再次连接，服务器端不会退出（epoll解除绑定、关闭client对象）;</li><li>新建一个client列表，存储每个客户端对象，循环遍历查看缓冲区有无数据；</li><li>文件协议设计，聊天室增加用户名功能。</li></ol>          </div><p><img src="https://s2.loli.net/2024/08/23/UaLvRpJhrT8jtC9.png" alt="聊天室测试结果"></p><h1 id="3-持续发送多个文件，协议设计"><a href="#3-持续发送多个文件，协议设计" class="headerlink" title="3.持续发送多个文件，协议设计"></a>3.持续发送多个文件，协议设计</h1><div class="note note-warning">            <p>使用TCP连接发送文件时，首先要以<strong>字节流</strong>形式传送，如果持续发送多个文件，文件名1+文件1内容+文件名2+文件2内容+。。。存在<strong>粘包</strong>问题，两次发送的报文挨在一起，分不开。</p>          </div><p>我们采用<strong>开火车的方式</strong>解决粘包，如下所示。</p><ol><li>小火车</li></ol><ul><li>火车头填写长度：字节数，python需pack为4字节整型数</li><li>火车车厢填写内容：字符串字节流</li></ul><table><thead><tr><th align="center"></th><th align="center">文件名</th><th align="center">文件内容</th></tr></thead><tbody><tr><td align="center">车头</td><td align="center">文件名长度(4B)</td><td align="center">文件内容总长度(4B)</td></tr><tr><td align="center">车厢</td><td align="center">文件名</td><td align="center">文件内容</td></tr></tbody></table><p><strong>Python的struct模块</strong>，提供了一种机制，能将int、float等基本数据类型打包成字符串（实际上相当于其他语言的字节流），可以在网络上传输，而接收端也可以通过解包还原出初始的数据。</p><ul><li><p>pack(fmt, var1, var2,…)<br>按照给定的格式(fmt)，把数据封装成字符串(实际上类似于C结构体的字节流);</p></li><li><p>unpack(fmt, string)<br>按照给定的格式(fmt)解析字节流(string)，<strong>返回</strong>解析出来的<strong>tuple</strong>元组;</p></li><li><p>calcsize(fmt)<br>计算给定的格式(fmt)占用多少字节的内存</p></li><li><p><a href="https://blog.csdn.net/yzy1103203312/article/details/78238004">Python中的struct模块</a></p></li></ul><h1 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h1><ul><li>写代码的时候对不确定的代码实现效果，<strong>自己动手写一个简单的例子</strong>验证一下就可以，这样就不至于代码写了一大堆，不确定错误在哪里。</li></ul><p><img src="https://s2.loli.net/2024/08/23/gY1eD3LfumUjVsb.png" alt=".png"></p></font>]]></content>
    
    
    
    <tags>
      
      <tag>socket</tag>
      
      <tag>epoll</tag>
      
      <tag>python</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
