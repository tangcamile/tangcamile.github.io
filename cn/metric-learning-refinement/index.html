<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="/img/cnblogpic.jpg"><link rel="icon" href="/img/cnblogpic.jpg"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="paopaotangzu"><meta name="keywords" content=""><meta name="description" content="few-shot learning的度量学习方法阅读与总结，目标是找到一篇文章，解决few-shot的以下问题：在基类上，模型学会区分相似和不相似的实例，从而提高下游任务的性能；支持集的样本过少，计算得到的类代表原型和实际期望得到的类原型有较大偏差，如何矫正有偏的类原型，进而提高模型分类精度。"><meta property="og:type" content="article"><meta property="og:title" content="度量学习思路整合"><meta property="og:url" content="http://paopaotangzu.xyz/cn/metric-learning-refinement/index.html"><meta property="og:site_name" content="Paopaotang&#39;s Blog"><meta property="og:description" content="few-shot learning的度量学习方法阅读与总结，目标是找到一篇文章，解决few-shot的以下问题：在基类上，模型学会区分相似和不相似的实例，从而提高下游任务的性能；支持集的样本过少，计算得到的类代表原型和实际期望得到的类原型有较大偏差，如何矫正有偏的类原型，进而提高模型分类精度。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://s2.loli.net/2024/12/09/qnV2rhwZTyjYGM8.png"><meta property="og:image" content="https://s2.loli.net/2025/01/09/NMQJKOybilHTAGV.png"><meta property="og:image" content="https://s2.loli.net/2025/01/09/eOXMZnHkxYScaRP.png"><meta property="og:image" content="https://s2.loli.net/2025/02/16/ClnkRoLwXu6gpDy.png"><meta property="article:published_time" content="2024-12-09T10:29:41.000Z"><meta property="article:modified_time" content="2025-02-24T10:53:49.799Z"><meta property="article:author" content="paopaotangzu"><meta property="article:tag" content="paper"><meta property="article:tag" content="metric-learning"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://s2.loli.net/2024/12/09/qnV2rhwZTyjYGM8.png"><title>度量学习思路整合 - Paopaotang&#39;s Blog</title><link rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><link rel="stylesheet" href="/css/iconfont.css"><link rel="stylesheet" href="/css/mac.css"><script id="fluid-configs">var Fluid=window.Fluid||{};Fluid.ctx=Object.assign({},Fluid.ctx);var dntVal,CONFIG={hostname:"paopaotangzu.xyz",root:"/",version:"1.9.7",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"left",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!0},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,follow_dnt:!0,baidu:null,google:{measurement_id:null},tencent:{sid:null,cid:null},woyaola:null,cnzz:null,leancloud:{app_id:"QBGbTI8cj2v6fHOtHIBRBIWM-gzGzoHsz",app_key:"TiLownaI18dqGnozzbpRqn4H",server_url:"https://qbgbti8c.lc-cn-n1-shared.com",path:"window.location.pathname",ignore_local:!1}},search_path:"/local-search.xml",include_content_in_search:!0};CONFIG.web_analytics.follow_dnt&&(dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on")))</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><script async>Fluid.ctx.dnt||Fluid.utils.createScript("https://www.googletagmanager.com/gtag/js?id=",function(){function a(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],a("js",new Date),a("config","")})</script><meta name="generator" content="Hexo 6.3.0"></head><body><header><div class="header-inner" style="height:50vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>Paopaotang&#39;s Blog</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/" target="_self"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/" target="_self"><i class="iconfont icon-archive-fill"></i> <span>归档</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/" target="_self"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/" target="_self"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/about/" target="_self"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item"><a class="nav-link" href="/links/" target="_self"><i class="iconfont icon-link-fill"></i> <span>友链</span></a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" target="_self" href="javascript:;" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><i class="iconfont icon-books"></i> <span>文档</span></a><div class="dropdown-menu" aria-labelledby="navbarDropdown"><a class="dropdown-item" href="https://hexo.fluid-dev.com/" target="_self"><span>主题博客</span> </a><a class="dropdown-item" href="https://hexo.fluid-dev.com/docs/guide/" target="_self"><span>配置指南</span> </a><a class="dropdown-item" href="https://hexo.fluid-dev.com/docs/icon/" target="_self"><span>图标用法</span></a></div></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(/img/pagepic.jpg) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.3)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="度量学习思路整合"></span></div><div class="mt-3"><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2024-12-09 18:29" pubdate>2024年12月9日 晚上</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 4.7k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 40 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-2"></div><div class="col-lg-8 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 id="seo-header">度量学习思路整合</h1><div class="markdown-body"><p>few-shot learning的度量学习方法阅读与总结，目标是找到一篇文章，解决few-shot的以下问题：<br>在基类上，模型学会区分相似和不相似的实例，从而提高下游任务的性能；<br>支持集的样本过少，计算得到的类代表原型和实际期望得到的类原型有较大偏差，如何矫正<strong>有偏的类原型</strong>，进而提高模型分类精度。</p><span id="more"></span><h2 id="1-Prototype-Rectification-for-Few-Shot-Learning"><a href="#1-Prototype-Rectification-for-Few-Shot-Learning" class="headerlink" title="1.Prototype Rectification for Few-Shot Learning"></a>1.Prototype Rectification for Few-Shot Learning</h2><h3 id="1-1-基本信息"><a href="#1-1-基本信息" class="headerlink" title="1.1 基本信息"></a>1.1 基本信息</h3><ul><li><p>2020年，ECCV会议论文</p></li><li><p>无开源源码</p></li><li><p>提出了如何减少类原型的偏置、减少support和query dataset之间的分布差。</p></li></ul><h3 id="1-2-方法记录"><a href="#1-2-方法记录" class="headerlink" title="1.2 方法记录"></a>1.2 方法记录</h3><h4 id="1-2-1-intra-class-bias"><a href="#1-2-1-intra-class-bias" class="headerlink" title="1.2.1 intra-class bias"></a>1.2.1 intra-class bias</h4><p>为了减少实际计算出的原型和真实原型之间的差距，采用<strong>伪标签策略</strong>，补充<code>support set</code>的样本，由此得到更接近真实的原型。具体操作是取<code>top-k</code>个置信度的未标记样本，加上伪标签，加入<code>support set</code>一起计算类代表原型，其中为了避免伪标签错分给原型带来大的误差，使用<strong>加权和的平均作为修改的原型</strong>，权重的计算公式：样本和basic prototypes有更大的余弦相似度，则在修改的原型中有更高占比。</p><div class="note note-info"><p>伪标签有一个使用前提，即需要一次性给出某个类的所有未标记样本，不适用于一个个给测试样本的情况。在脑电身份识别上，一个人作为一个类，<strong>他的测试样本是否可以一次性获取到？</strong>，决定了伪标签是否适用。</p></div><h4 id="1-2-2-cross-class-bias"><a href="#1-2-2-cross-class-bias" class="headerlink" title="1.2.2 cross-class bias"></a>1.2.2 cross-class bias</h4><p>首先两个set被假设为分布在同一个domain中，但<code>support set</code>和<code>quary set</code>之间存在<strong>分布差</strong>，提出为了减少两者的分布差，可以把<code>quary set</code>朝<code>support set</code>移动。具体地，文章提出给每个标准化后的quary feature$\overline{X_q}$添加一个转换参数epilon。<br><img src="https://s2.loli.net/2024/12/09/qnV2rhwZTyjYGM8.png" srcset="/img/loading.gif" lazyload alt="image.png"></p><div class="note note-info"><p>减小分布差，脑电身份识别，support set用的是登记session的样本，quary set可能用的是另一个session的样本，由于时变性，两者的分布差肯定是存在的，甚至于同一个session，随着人状态的波动，样本之间估计也存在明显的分布差，这个方法可以一试。</p></div><h2 id="2-Free-Lunch-for-Few-shot-Learning-Distribution-Calibration"><a href="#2-Free-Lunch-for-Few-shot-Learning-Distribution-Calibration" class="headerlink" title="2.Free Lunch for Few-shot Learning: Distribution Calibration"></a>2.Free Lunch for Few-shot Learning: Distribution Calibration</h2><h3 id="2-1-基本信息"><a href="#2-1-基本信息" class="headerlink" title="2.1 基本信息"></a>2.1 基本信息</h3><ul><li><p>2021年，ICLR会议论文</p></li><li><p>源码：<a target="_blank" rel="noopener" href="https://github.com/ShuoYang-1998/Few_Shot_Distribution_Calibration">Few_Shot_Distribution_Calibration</a></p></li><li><p>提出从语义相似的基类(s)迁移统计数据来校准这些少数样本类的分布，接着<strong>依据新分布的均值和方差随机采样一定数量的样本</strong>，对novel classes的n_way k_shot任务的支持集进行补充，补充后的support set aug输入分类器fit。</p></li></ul><h3 id="2-2-方法记录"><a href="#2-2-方法记录" class="headerlink" title="2.2 方法记录"></a>2.2 方法记录</h3><p>前提假设：假设特征嵌入的每个维度都服从<strong>高斯分布</strong>。</p><p>这篇代码<strong>基类</strong>是有充足样本的类，用别人的SOTA分类模型，取倒数第二层作为特征提取器，用于提取基类和novel类的特征嵌入。接着计算每个基类特征层面的均值向量和协方差。</p><p>测试类(novel classes)用的是轮次训练的方式，例如：</p><div class="fold"><div class="fold-title fold-info collapsed" data-toggle="collapse" href="#collapse-8f7bdefe" role="button" aria-expanded="false" aria-controls="collapse-8f7bdefe"><div class="fold-arrow">▶</div>FSLTask参数</div><div class="fold-collapse collapse" id="collapse-8f7bdefe"><div class="fold-content"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs PYTHON"><span class="hljs-comment"># ---- data loading</span><br>    dataset = <span class="hljs-string">&#x27;miniImagenet&#x27;</span><br>    n_shot = <span class="hljs-number">1</span><br>    n_ways = <span class="hljs-number">5</span><br>    n_queries = <span class="hljs-number">15</span><br>    n_runs = <span class="hljs-number">10000</span><br>    n_lsamples = n_ways * n_shot<br>    n_usamples = n_ways * n_queries<br>    n_samples = n_lsamples + n_usamples<br></code></pre></td></tr></table></figure></div></div></div><p>每次从novel classes中抽5个类，每个类有1个support sample，15个query sample。每一次run，对抽取的5个support_data(先转成特征嵌入)分别做分布校准，然后生成若干个数的特征向量作为support集的<strong>补充</strong>，一起输入分类器fit，在query samples上测试，计算acc，最后取10000次run的平均acc。</p><p>分布校准core代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs PYTHON"><span class="hljs-keyword">def</span> <span class="hljs-title function_">distribution_calibration</span>(<span class="hljs-params">query, base_means, base_cov, k,alpha=<span class="hljs-number">0.21</span></span>):<br>    dist = []  <span class="hljs-comment"># 计算support sample(变量名叫query)和mean之间的欧式距离</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(base_means)):<br>        dist.append(np.linalg.norm(query-base_means[i]))<br>    index = np.argpartition(dist, k)[:k]  <span class="hljs-comment"># 从数组 dist 中找到前𝑘小的元素的索引</span><br>    mean = np.concatenate([np.array(base_means)[index], query[np.newaxis, :]])<br>    calibrated_mean = np.mean(mean, axis=<span class="hljs-number">0</span>)<br>    calibrated_cov = np.mean(np.array(base_cov)[index], axis=<span class="hljs-number">0</span>)+alpha<br><br>    <span class="hljs-keyword">return</span> calibrated_mean, calibrated_cov<br></code></pre></td></tr></table></figure><div class="note note-primary"><ol><li>这篇文章在<strong>细粒度数据集CUB</strong>上也应用了DC方法，有超过200种不同的鸟类图片。每个人的脑电虽然说各自有判别性特征，其实还是很相似的细粒度数据，这个方法通过迁移<span class="label label-secondary">k</span>个距离最小的基类的分布，来直接增加支持集的样本，进而提高分类性能。</li><li>要调的超参数多，采样个数、k个基类、离散程度a。</li><li>在脑电上是否有效呢？特征向量都是经过标准化&#x2F;幂变换的。</li></ol></div><h2 id="3-Semantic-Based-Implicit-Feature-Transform-for-Few-Shot-Classification"><a href="#3-Semantic-Based-Implicit-Feature-Transform-for-Few-Shot-Classification" class="headerlink" title="3.Semantic-Based Implicit Feature Transform for Few-Shot Classification"></a>3.Semantic-Based Implicit Feature Transform for Few-Shot Classification</h2><h3 id="3-1-基本信息"><a href="#3-1-基本信息" class="headerlink" title="3.1 基本信息"></a>3.1 基本信息</h3><ul><li><p>2024年，International Journal of Computer Vision</p></li><li><p>源码：<a target="_blank" rel="noopener" href="https://github.com/pmhDL/SIFT.git">SIFT</a></p></li><li><p>借鉴的是前面分布矫正的论文，同样是通过从基类补充特征向量到novel类上面去，不同的是图像类别标签（如cat、dog），<strong>本身具有语义信息</strong>，可以用不同的词向量来表达。然后通过语义嵌入的相似度来选择最近的基类，而不是前面通过统计特征。</p></li></ul><h3 id="3-2-方法记录"><a href="#3-2-方法记录" class="headerlink" title="3.2 方法记录"></a>3.2 方法记录</h3><ul><li><p>脑电的类别标签纯粹是<span class="label label-secondary">'Person A'</span>、<span class="label label-secondary">'Person B'</span>的形式，并不具有语义信息，所以如果要用，应该采用Free Lunch的方法<strong>做跨时段的分布矫正</strong>。</p></li><li><p>提出了一种原型矫正的方法。适用于transductive setting，即查询样本全部一次给出的情况。通过K-means把查询样本分簇为N类，接着建立路径规划问题的数学模型，为这N个类别确定互相不重复的标签，与初始的每个类原型做平均。</p></li></ul><div class="fold"><div class="fold-title fold-info collapsed" data-toggle="collapse" href="#collapse-6ae3e036" role="button" aria-expanded="false" aria-controls="collapse-6ae3e036"><div class="fold-arrow">▶</div>原型矫正实现</div><div class="fold-collapse collapse" id="collapse-6ae3e036"><div class="fold-content"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs PYTHON"><span class="hljs-keyword">def</span> <span class="hljs-title function_">updateproto</span>(<span class="hljs-params">Xs, ys, cls_center, way</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;更新原型 (prototype)，并与聚类中心 (cluster center) 结合&quot;&quot;&quot;</span><br>    <br>    <span class="hljs-comment"># 调用 np_proto 计算每个类别的原型</span><br>    proto = np_proto(Xs, ys, way)  <br>    <br>    <span class="hljs-comment"># 计算每个类别原型与所有聚类中心之间的平方欧式距离</span><br>    <span class="hljs-comment"># 使用广播机制，扩展 proto 和 cls_center 的维度后相减</span><br>    dist = ((proto[:, np.newaxis, :] - cls_center[np.newaxis, :, :]) ** <span class="hljs-number">2</span>).<span class="hljs-built_in">sum</span>(<span class="hljs-number">2</span>)  <br>    <br>    <span class="hljs-comment"># 找到每个类别原型距离最近的聚类中心索引</span><br>    <span class="hljs-built_in">id</span> = dist.argmin(<span class="hljs-number">1</span>)  <br>    feat_proto = np.zeros((way, Xs.shape[<span class="hljs-number">1</span>]))  <br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(way):<br>        <span class="hljs-comment"># 将当前类别的原型和其最近的聚类中心取平均值</span><br>        feat_proto[i] = (cls_center[<span class="hljs-built_in">id</span>[i]] + proto[i]) / <span class="hljs-number">2</span>  <br>    <br>    <span class="hljs-keyword">return</span> feat_proto  <br></code></pre></td></tr></table></figure></div></div></div><h2 id="4-Matching-Feature-Sets-for-Few-Shot-Image-Classification"><a href="#4-Matching-Feature-Sets-for-Few-Shot-Image-Classification" class="headerlink" title="4.Matching Feature Sets for Few-Shot Image Classification"></a>4.Matching Feature Sets for Few-Shot Image Classification</h2><h3 id="4-1-基本信息"><a href="#4-1-基本信息" class="headerlink" title="4.1 基本信息"></a>4.1 基本信息</h3><ul><li><p>2022年，CVPR会议论文</p></li><li><p>源码：<a target="_blank" rel="noopener" href="https://lvsn.github.io/SetFeat/">SetFeat-fs</a></p></li><li><p>以Conv4-64骨干网络为例，提出把一张图像输入进去，经过一个Block，就输出一个经<strong>自注意力机制mapper</strong>计算后的特征向量$h_m$，这样处理从一张图片中提取出一组m(4)个特征向量。距离度量用的是负余弦相似度，实际上和原型网络的距离度量很相似，区分在于有多个特征向量，它这里<strong>统一了向量的shape</strong>，相当于在同一个特征空间内，按mapper聚合多个类中心。</p></li></ul><h3 id="4-2-方法记录"><a href="#4-2-方法记录" class="headerlink" title="4.2 方法记录"></a>4.2 方法记录</h3><ul><li><p>如何利用多个不同层提取出的特征向量，核心如下公式所示。<br><img src="https://s2.loli.net/2025/01/09/NMQJKOybilHTAGV.png" srcset="/img/loading.gif" lazyload alt="image.png"></p></li><li><p><strong>训练流程：</strong>两个阶段，<span class="label label-primary">第一阶段</span>就是正常带FC层的分类器，并且本文是在每个Block后面都接一个FC层，分别训练到这个Block为止的网络，<span class="label label-primary">第二阶段</span>，舍去FC层，应用轮次训练在基类上模拟FSL Task，通过公式6的负对数概率计算loss，反向传播微调编码器的参数。微调结束，最终在novel类上面推理，之前的Free Lunch也是一样的流程。<br><img src="https://s2.loli.net/2025/01/09/eOXMZnHkxYScaRP.png" srcset="/img/loading.gif" lazyload alt="image.png"></p></li></ul><div class="note note-secondary"><ol><li>总结一下，它提高原型网络分类精度的手段就是，同一个特征space，一个类别聚合m个类中心。</li><li>感觉第一阶段训练就相当于独立地训练了4个encoder，但是又不独立，前面的参数是要重复使用的，具体要看代码train部分。</li></ol></div><div class="note note-info"><ol><li>这种在预训练阶段，训练一个分类器的做法和我之前看的<strong>有监督对比学习</strong>训练编码器的方式不一样，我的想法是也许可以借用这个metric，然后用SCL的做法分别独立训练出3个encoder。之后可以有两种metric方法，一种是和本文做法一样，投射到同一个特征空间；另一种映射到不同特征空间分别做相似度计算，再求和，作为新的metric。</li><li>还有一个想法是SCL的<strong>encoder很关键，</strong>决定了特征向量的质量，借鉴GoogleNet的多尺度卷积方法，预训练出一个encoder，也可以试一下。</li></ol></div><h3 id="4-3-参考博客"><a href="#4-3-参考博客" class="headerlink" title="4.3 参考博客"></a>4.3 参考博客</h3><ul><li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43499457/article/details/124595010?ops_request_misc=%257B%2522request%255Fid%2522%253A%25223c42cb44ac6d1d4ee40531845d5a092b%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=3c42cb44ac6d1d4ee40531845d5a092b&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-1-124595010-null-null.142%5Ev101%5Epc_search_result_base8&utm_term=%E7%9B%91%E7%9D%A3%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0&spm=1018.2226.3001.4187">有监督对比学习在分类任务中的应用 Supervised Contrastive Learning</a></p></li><li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/c___c18/article/details/144056112?ops_request_misc=&request_id=&biz_id=102&utm_term=%E7%9B%91%E7%9D%A3%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-1-144056112.142%5Ev101%5Epc_search_result_base8&spm=1018.2226.3001.4187">监督对比学习代码实现与分析（Supervised Contrastive Learning in NLP）</a></p></li><li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/Bluebro/article/details/130035707">主干网络backbone讲解—— Conv4与Resnet12</a></p></li></ul><h2 id="5-BSNet-Bi-Similarity-Network-for-Few-shot-Fine-grained-Image-Classification"><a href="#5-BSNet-Bi-Similarity-Network-for-Few-shot-Fine-grained-Image-Classification" class="headerlink" title="5.BSNet: Bi-Similarity Network for Few-shot  Fine-grained Image Classification"></a>5.BSNet: Bi-Similarity Network for Few-shot Fine-grained Image Classification</h2><h3 id="5-1-基本信息"><a href="#5-1-基本信息" class="headerlink" title="5.1 基本信息"></a>5.1 基本信息</h3><ul><li><p>2020年，‌IEEE Transactions on Image Processing（TIP）期刊论文</p></li><li><p>源码：<a target="_blank" rel="noopener" href="https://github.com/PRIS-CV/BSNet">BSNet</a></p></li><li><p>代码格式简洁，是基于度量学习的图像细粒度分类，提出同时使用余弦相似度和欧式距离两种loss，即在ProtoNet上添加一个余弦相似度的loss，分类精度在图像分类上有所提高。</p></li></ul><div class="note note-light"><p>最后调优模型的时候可以试一下，有空看代码。</p></div><h2 id="6-Supervised-Contrastive-Learning"><a href="#6-Supervised-Contrastive-Learning" class="headerlink" title="6.Supervised Contrastive Learning"></a>6.Supervised Contrastive Learning</h2><h3 id="6-1-基本信息"><a href="#6-1-基本信息" class="headerlink" title="6.1 基本信息"></a>6.1 基本信息</h3><ul><li><p>2020年，NeurIPS</p></li><li><p>源码：<a target="_blank" rel="noopener" href="https://github.com/HobbitLong/SupContrast">SupContrast</a></p></li><li><p>SCL集合了传统度量学习领域的<span class="label label-secondary">Triplet loss</span>、<span class="label label-secondary">N-pair loss</span>两者的特点，提出对一个Anchor除了<strong>考虑多个负样例之外，也同时考虑多个正样例</strong>，设计的损失函数避开了需要显式地调参以挖掘半难样本的需求。</p></li></ul><h3 id="6-2-方法记录"><a href="#6-2-方法记录" class="headerlink" title="6.2 方法记录"></a>6.2 方法记录</h3><ul><li><p>在一个batch中，每一个样本$x$都经过一个数据增强模块$Aug(·)$生成两个随机增强$\bar{x} &#x3D; Aug(x)$，两个增强后的样本标签一样，属于同一个类别。接下来<strong>如何保证随机生成的一个batch有多个样本标签相同呢</strong>，除了数据增强的方式生成正样本对之外，相对于类别个数大小C，batch的<code>batch_size = N</code>要远大于C，这样平均N&#x2F;C，就必定会采样到同一个类的多个样本。</p></li><li><p>代码与对应公式的解释：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/670579496">监督对比学习SupConLoss代码学习笔记</a></p></li></ul><div class="fold"><div class="fold-title fold-info collapsed" data-toggle="collapse" href="#collapse-9c51fa20" role="button" aria-expanded="false" aria-controls="collapse-9c51fa20"><div class="fold-arrow">▶</div>SupContrastLoss</div><div class="fold-collapse collapse" id="collapse-9c51fa20"><div class="fold-content"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><code class="hljs PYTHON"><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Author: Yonglong Tian (yonglong@mit.edu)</span><br><span class="hljs-string">Date: May 07, 2020</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-keyword">from</span> __future__ <span class="hljs-keyword">import</span> print_function<br><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SupConLoss</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.</span><br><span class="hljs-string">    It also supports the unsupervised contrastive loss in SimCLR&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, temperature=<span class="hljs-number">0.07</span>, contrast_mode=<span class="hljs-string">&#x27;all&#x27;</span>,</span><br><span class="hljs-params">                 base_temperature=<span class="hljs-number">0.07</span></span>):<br>        <span class="hljs-built_in">super</span>(SupConLoss, self).__init__()<br>        self.temperature = temperature<br>        self.contrast_mode = contrast_mode<br>        self.base_temperature = base_temperature<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, features, labels=<span class="hljs-literal">None</span>, mask=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Compute loss for model. If both `labels` and `mask` are None,</span><br><span class="hljs-string">        it degenerates to SimCLR unsupervised loss:</span><br><span class="hljs-string">        https://arxiv.org/pdf/2002.05709.pdf</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            features: hidden vector of shape [bsz, n_views, ...].</span><br><span class="hljs-string">            labels: ground truth of shape [bsz].</span><br><span class="hljs-string">            mask: contrastive mask of shape [bsz, bsz], mask_&#123;i,j&#125;=1 if sample j</span><br><span class="hljs-string">                has the same class as sample i. Can be asymmetric.</span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            A loss scalar.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        device = (torch.device(<span class="hljs-string">&#x27;cuda&#x27;</span>)<br>                  <span class="hljs-keyword">if</span> features.is_cuda<br>                  <span class="hljs-keyword">else</span> torch.device(<span class="hljs-string">&#x27;cpu&#x27;</span>))<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(features.shape) &lt; <span class="hljs-number">3</span>:<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;`features` needs to be [bsz, n_views, ...],&#x27;</span><br>                             <span class="hljs-string">&#x27;at least 3 dimensions are required&#x27;</span>)<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(features.shape) &gt; <span class="hljs-number">3</span>:<br>            features = features.view(features.shape[<span class="hljs-number">0</span>], features.shape[<span class="hljs-number">1</span>], -<span class="hljs-number">1</span>)<br><br>        batch_size = features.shape[<span class="hljs-number">0</span>]  <span class="hljs-comment"># batch_size = N</span><br>        <span class="hljs-keyword">if</span> labels <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Cannot define both `labels` and `mask`&#x27;</span>)<br>        <span class="hljs-keyword">elif</span> labels <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> mask <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            mask = torch.eye(batch_size, dtype=torch.float32).to(device)<br>        <span class="hljs-keyword">elif</span> labels <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            labels = labels.contiguous().view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>            <span class="hljs-keyword">if</span> labels.shape[<span class="hljs-number">0</span>] != batch_size:<br>                <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Num of labels does not match num of features&#x27;</span>)<br>            mask = torch.eq(labels, labels.T).<span class="hljs-built_in">float</span>().to(device)<br>        <span class="hljs-keyword">else</span>:<br>            mask = mask.<span class="hljs-built_in">float</span>().to(device)<br><br>        contrast_count = features.shape[<span class="hljs-number">1</span>]  <span class="hljs-comment"># 视图个数</span><br>        contrast_feature = torch.cat(torch.unbind(features, dim=<span class="hljs-number">1</span>), dim=<span class="hljs-number">0</span>)  <span class="hljs-comment"># 消除视图维度，按batch拼接，得到[N*n_views, feature_dim]</span><br>        <span class="hljs-keyword">if</span> self.contrast_mode == <span class="hljs-string">&#x27;one&#x27;</span>:<br>            anchor_feature = features[:, <span class="hljs-number">0</span>]  <span class="hljs-comment"># 只取第一个视图作为anchor</span><br>            anchor_count = <span class="hljs-number">1</span><br>        <span class="hljs-keyword">elif</span> self.contrast_mode == <span class="hljs-string">&#x27;all&#x27;</span>:<br>            anchor_feature = contrast_feature  <span class="hljs-comment"># 所有视图作为anchor</span><br>            anchor_count = contrast_count<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Unknown mode: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(self.contrast_mode))<br><br>        <span class="hljs-comment"># compute logits</span><br>        anchor_dot_contrast = torch.div(<br>            torch.matmul(anchor_feature, contrast_feature.T),  <span class="hljs-comment"># 1.单个 [N, ft_dim] * [ft_dim, N*n_views] = [N, N*n_views]  2.所有 [N*n_views, ft_dim] * [ft_dim, N*n_views] = [N*n_views, N*n_views]</span><br>            self.temperature)<br>        <span class="hljs-comment"># for numerical stability</span><br>        logits_max, _ = torch.<span class="hljs-built_in">max</span>(anchor_dot_contrast, dim=<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>        logits = anchor_dot_contrast - logits_max.detach()<br><br>        <span class="hljs-comment"># tile mask</span><br>        mask = mask.repeat(anchor_count, contrast_count)<br>        <span class="hljs-comment"># mask-out self-contrast cases</span><br>        logits_mask = torch.scatter(<br>            torch.ones_like(mask),<br>            <span class="hljs-number">1</span>,<br>            torch.arange(batch_size * anchor_count).view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>).to(device),<br>            <span class="hljs-number">0</span><br>        )<br>        mask = mask * logits_mask<br><br>        <span class="hljs-comment"># compute log_prob</span><br>        exp_logits = torch.exp(logits) * logits_mask  <span class="hljs-comment"># [N*n_views, N*n_views]</span><br>        log_prob = logits - torch.log(exp_logits.<span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>))  <span class="hljs-comment"># [N*n_views, N*n_views]</span><br><br>        <span class="hljs-comment"># compute mean of log-likelihood over positive</span><br>        <span class="hljs-comment"># modified to handle edge cases when there is no positive pair</span><br>        <span class="hljs-comment"># for an anchor point.</span><br>        <span class="hljs-comment"># Edge case e.g.:-</span><br>        <span class="hljs-comment"># features of shape: [4,1,...]</span><br>        <span class="hljs-comment"># labels:            [0,1,1,2]</span><br>        <span class="hljs-comment"># loss before mean:  [nan, ..., ..., nan]</span><br>        mask_pos_pairs = mask.<span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span>)<br>        mask_pos_pairs = torch.where(mask_pos_pairs &lt; <span class="hljs-number">1e-6</span>, <span class="hljs-number">1</span>, mask_pos_pairs)<br>        mean_log_prob_pos = (mask * log_prob).<span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span>) / mask_pos_pairs  <span class="hljs-comment"># [N*n_views]</span><br><br>        <span class="hljs-comment"># loss</span><br>        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos <span class="hljs-comment"># [N*n_views]</span><br>        loss = loss.view(anchor_count, batch_size).mean()  <span class="hljs-comment"># 标量</span><br><br>        <span class="hljs-keyword">return</span> loss<br></code></pre></td></tr></table></figure></div></div></div><div class="note note-info"><p>仿照这篇论文的三个核心模块：<strong>数据增强模块</strong>、编码网络、映射网络，计划对脑电原始数据做Channel Reflection数据增强，以及一个其他数据增强操作，和本文的监督对比loss匹配，Backbone也需要找一个替换。</p></div><h2 id="7-Channel-reflection-Knowledge-driven-data-augmentation-for-EEG-based-brain–computer-interfaces"><a href="#7-Channel-reflection-Knowledge-driven-data-augmentation-for-EEG-based-brain–computer-interfaces" class="headerlink" title="7.Channel reflection: Knowledge-driven data augmentation for EEG-based brain–computer interfaces"></a>7.Channel reflection: Knowledge-driven data augmentation for EEG-based brain–computer interfaces</h2><h3 id="7-1-基本信息"><a href="#7-1-基本信息" class="headerlink" title="7.1 基本信息"></a>7.1 基本信息</h3><ul><li><p>2024年，Neural Networks</p></li><li><p>源码：<a target="_blank" rel="noopener" href="https://github.com/wzwvv/EEGAug">EEGAug</a></p></li><li><p>提出了一种无需超参数的<strong>通道交换(CR)数据增强</strong>方法，传统的数据增强如添加Noise、Scale、Frep都需要调超参数，并且十分鲁棒，在MI、SSVEP、ERP、癫痫检测4个实验范式下均适用，相比于Baseline(没有数据增强)，分类精度更好。注意经CR数据增强之后，训练数据翻倍。</p></li></ul><h3 id="7-2-方法记录"><a href="#7-2-方法记录" class="headerlink" title="7.2 方法记录"></a>7.2 方法记录</h3><div class="fold"><div class="fold-title fold-info collapsed" data-toggle="collapse" href="#collapse-05eef982" role="button" aria-expanded="false" aria-controls="collapse-05eef982"><div class="fold-arrow">▶</div>leftrightflipping_transform</div><div class="fold-collapse collapse" id="collapse-05eef982"><div class="fold-content"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs PYTHON"><span class="hljs-keyword">def</span> <span class="hljs-title function_">leftrightflipping_transform</span>(<span class="hljs-params">X, left_mat, right_mat</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Parameters</span><br><span class="hljs-string">    ----------</span><br><span class="hljs-string">    X: torch tensor of shape (num_samples, 1, num_channels, num_timesamples)</span><br><span class="hljs-string">    left_mat: numpy array of shape (a, ), where a is the number of left brain channels, in order</span><br><span class="hljs-string">    right_mat: numpy array of shape (b, ), where b is the number of right brain channels, in order</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns</span><br><span class="hljs-string">    -------</span><br><span class="hljs-string">    transformedX: transformed signal of torch tensor of shape (num_samples, num_channels, num_timesamples)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    num_samples, _, num_channels, num_timesamples = X.shape<br>    transformedX = torch.zeros((num_samples, <span class="hljs-number">1</span>, num_channels, num_timesamples))<br>    <span class="hljs-keyword">for</span> ch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_channels):<br>        <span class="hljs-keyword">if</span> ch <span class="hljs-keyword">in</span> left_mat:<br>            ind = left_mat.index(ch)<br>            transformedX[:, <span class="hljs-number">0</span>, ch, :] = X[:, <span class="hljs-number">0</span>, right_mat[ind], :]<br>        <span class="hljs-keyword">elif</span> ch <span class="hljs-keyword">in</span> right_mat:<br>            ind = right_mat.index(ch)<br>            transformedX[:, <span class="hljs-number">0</span>, ch, :] = X[:, <span class="hljs-number">0</span>, left_mat[ind], :]<br>        <span class="hljs-keyword">else</span>:<br>            transformedX[:, <span class="hljs-number">0</span>, ch, :] = X[:, <span class="hljs-number">0</span>, ch, :]<br><br>    <span class="hljs-keyword">return</span> transformedX<br></code></pre></td></tr></table></figure></div></div></div><ul><li>即根据脑电极通道的索引，位于中线的电极不变，左脑和右脑对称分布的电极做一个交换，类似于图像的翻转操作。除左右手MI想象要调换标签之外，ERP不需要换标签。调用方法如下：</li></ul><div class="fold"><div class="fold-title fold-info collapsed" data-toggle="collapse" href="#collapse-90469042" role="button" aria-expanded="false" aria-controls="collapse-90469042"><div class="fold-arrow">▶</div>调用example</div><div class="fold-collapse collapse" id="collapse-90469042"><div class="fold-content"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs PYTHON"><span class="hljs-keyword">if</span> dataset == <span class="hljs-string">&#x27;BNCI2014001&#x27;</span>:<br>    left_mat = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">13</span>, <span class="hljs-number">14</span>, <span class="hljs-number">18</span>]<br>    right_mat = [<span class="hljs-number">5</span>, <span class="hljs-number">4</span>, <span class="hljs-number">12</span>, <span class="hljs-number">11</span>, <span class="hljs-number">10</span>, <span class="hljs-number">17</span>, <span class="hljs-number">16</span>, <span class="hljs-number">20</span>]<br>    aug_train_x = leftrightflipping_transform(<br>        torch.from_numpy(train_x).to(torch.float32).reshape(train_x.shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>, ch_num, -<span class="hljs-number">1</span>),<br>        left_mat, right_mat).numpy().reshape(train_x.shape[<span class="hljs-number">0</span>], ch_num, -<span class="hljs-number">1</span>)<br>    aug_train_y = <span class="hljs-number">1</span> - train_y  <span class="hljs-comment"># 二分类标签0-1对调</span><br></code></pre></td></tr></table></figure></div></div></div><ul><li><a target="_blank" rel="noopener" href="https://roses.blog.csdn.net/article/details/141632317?spm=1001.2101.3001.6650.3&utm_medium=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-3-141632317-blog-128355505.235%5Ev43%5Epc_blog_bottom_relevance_base4&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~BlogCommendFromBaidu~Rate-3-141632317-blog-128355505.235%5Ev43%5Epc_blog_bottom_relevance_base4&utm_relevant_index=6">​华中科技大学伍冬睿团队提出知识数据融合的通道交换脑电数据增强方法</a></li></ul><h2 id="8-Revisiting-Prototypical-Network-for-Cross-Domain-Few-Shot-Learning"><a href="#8-Revisiting-Prototypical-Network-for-Cross-Domain-Few-Shot-Learning" class="headerlink" title="8.Revisiting Prototypical Network for Cross Domain Few-Shot Learning"></a>8.Revisiting Prototypical Network for Cross Domain Few-Shot Learning</h2><h3 id="8-1-基本信息"><a href="#8-1-基本信息" class="headerlink" title="8.1 基本信息"></a>8.1 基本信息</h3><ul><li><p>2023年，CVPR会议论文</p></li><li><p>源码：<a target="_blank" rel="noopener" href="https://github.com/NWPUZhoufei/LDP-Net">LDP-Net</a></p></li><li><p>是基于baseline原型网络的改进，指出全局特征提取器倾向于学习浅显的颜色、形状等简单特征，而<strong>没有学习到这个类不变的、深层的所谓“语义特征”</strong>，作者由此提出了局部全局蒸馏-原型网络，通过建立一个两分支网络对查询图像和其<strong>局部随机多个裁剪增强</strong>进行分类，并利用<strong>知识蒸馏</strong>强制这两个分支保持类原型预测结果一致，使得特征提取网络学到更多的语义信息。</p></li></ul><h3 id="8-2-方法记录"><a href="#8-2-方法记录" class="headerlink" title="8.2 方法记录"></a>8.2 方法记录</h3><ul><li><p>方法在消融实验的结果上可以看出，主要是<strong>局部裁剪的图像要和原始的图像</strong>类标签保持一致这一步起作用，也就是<code>self-image distill</code>，并且“知识蒸馏”是包装过的说法，实际上方法的有效性建立在图像经随机裁剪之后，能关注到更多的细节信息，结合这个分支和原始的整张图像分支，两者共同对特征提取器的参数做反向传播，共同发挥作用。</p></li><li><p>特征提取器首先经预训练。接着值得注意的是为了不同时训练两个分支的模型参数，并且以全局图像为主干，本文提出了用<span class="label label-secondary">Exponential Moving Average (EMA)</span>，一种<strong>加权移动的方法</strong>来更新局部分支的模型参数。<br><img src="https://s2.loli.net/2025/02/16/ClnkRoLwXu6gpDy.png" srcset="/img/loading.gif" lazyload alt="模型训练图"></p></li></ul><h2 id="9-Cross-session-SSVEP-brainprint-recognition-using-attentive-multi-subband-depth-identity-embedding-learning-network"><a href="#9-Cross-session-SSVEP-brainprint-recognition-using-attentive-multi-subband-depth-identity-embedding-learning-network" class="headerlink" title="9.Cross-session SSVEP brainprint recognition using attentive multi-subband depth identity embedding learning network"></a>9.Cross-session SSVEP brainprint recognition using attentive multi-subband depth identity embedding learning network</h2><h3 id="9-1-基本信息"><a href="#9-1-基本信息" class="headerlink" title="9.1 基本信息"></a>9.1 基本信息</h3><ul><li><p>2024年，Cognitive Neurodynamics</p></li><li><p>使用2s的样本，仅利用1个session的数据做训练，训练和验证集比例设置为8:2，在另一个session上检验有效性。模块包括Deepconvwise、Res2Net、时空注意力、<strong>Attentive Statistic Pooling</strong>。</p></li></ul><h2 id="10-TST-MFL-Two-stage-training-based-metric-fusion-learning-for-few-shot-image-classification"><a href="#10-TST-MFL-Two-stage-training-based-metric-fusion-learning-for-few-shot-image-classification" class="headerlink" title="10.TST_MFL: Two-stage training based metric fusion learning for few-shot image classification"></a>10.TST_MFL: Two-stage training based metric fusion learning for few-shot image classification</h2><h3 id="10-1-基本信息"><a href="#10-1-基本信息" class="headerlink" title="10.1 基本信息"></a>10.1 基本信息</h3><ul><li><p>2024年，Information Fusion</p></li><li><p>源码：<a target="_blank" rel="noopener" href="https://github.com/ZitZhengWang/TST_MFL">TST_MFL</a></p></li><li><p>传统的度量学习方法大多只提取<strong>全局特征表示</strong>，一些在元训练阶段结合了<strong>局部特征表示</strong>，而本文提出不仅要在元训练阶段特征融合，也要在预训练阶段特征融合，进一步提高特征的判别性。</p></li></ul><h3 id="10-2-方法记录"><a href="#10-2-方法记录" class="headerlink" title="10.2 方法记录"></a>10.2 方法记录</h3><h4 id="10-2-1-Pre-training-Stage"><a href="#10-2-1-Pre-training-Stage" class="headerlink" title="10.2.1 Pre-training Stage"></a>10.2.1 Pre-training Stage</h4><ul><li>提出了一个双路（dual-path）的预训练网络，采用传统的交叉熵损失在基类上训练一个多分类任务，具体的：<ol><li>全局分类subnet：图像尺寸为84x84，backbone采用ResNet12用于特征提取，最后接全连接层和Softmax计算全局分类损失；</li><li>局部分类subnet：图像尺寸为26x26，backbone也是ResNet12用于局部图像块的特征提取，接了一个8头自注意力层，然后全连接层加Softmax。假设一张图像分割为36块，这36块都属于同一个类别，标签的范围和全局一样是D类，则这36小块每块分别提取特征，独立预测分类标签，计算一个局部分类损失。</li><li>损失函数1和2：全局交叉熵损失$L_{CE}$、平均局部交叉熵损失$L_{LCE}$</li><li>损失函数3：<strong>预测蒸馏损失</strong>$L_{KD}$，改进了知识蒸馏的<strong>KL散度</strong>，让老师（全局预测出来的类标签概率分布）和学生（局部预测出来的平均类标签概率分布）能够相互学习，通过对这两个概率分布取平均，然后作为老师，指导前面两个概率分布靠近这个老师。</li><li>损失函数4：<strong>局部多样性损失</strong>$L_B$，目的是最小化同一张图像的局部块之间的预测概率分布差异。</li></ol></li></ul><h4 id="10-2-2-Meta-training-Stage"><a href="#10-2-2-Meta-training-Stage" class="headerlink" title="10.2.2 Meta-training Stage"></a>10.2.2 Meta-training Stage</h4><ul><li>提出了一个全局-局部度量融合网络，在基类上以轮次训练的方式继续提高小样本分类性能，具体的：<ol><li>保留预训练网络的全局子网特征提取器、局部子网的特征提取器、self-attention模块；</li><li>Global metric subnet：在一个小样本任务中，Support集按照类别计算类原型，Query集中的每个样本转为特征之后，利用<strong>余弦相似度</strong>来计算和每个类原型（N类）的相似度，最后一个Query就是一条相似度向量：$Sim_g \in R^N$；</li><li>Local metric subnet：<ul><li>输入：支持集、查询集样本的局部裁剪块作为输入，每个样本裁剪为$m$块；</li><li>由于可能裁剪到和关键分类信息不相关的背景图像，提出了一个<strong>基于相似度阈值</strong>的局部特征过滤模块，即把局部块提取出的特征和这张图像的全局特征进行余弦相似度对比，如果低于阈值0.2，则把这个局部特征<strong>清零</strong>，达到过滤的效果；</li><li>接着计算相似度，支持集<strong>每类</strong>K个样本，每个样本拆为m个，则一共有Km个局部块，每个经过过滤的Query局部块为m个。挨个计算<strong>余弦相似度</strong>，得到余弦相似度矩阵，即km行m列，然后根据KNN为每个Query局部块（共m个）选出前k个值最大的相似度，求和最后取平均，作为最终的该Query样本和该类的相似度。</li><li>1个Query对应N类，有n个相似度值，组成一条相似度向量$Sim_l$。</li></ul></li><li>Metric Fusion module： 两个权重是可学习的超参数。<br>$$Sim_f &#x3D; w_1 * Sim_g + w_2 * Sim_l$$</li><li>标签的预测最后采用公式：<br>$$\hat{y} &#x3D; \arg\max\limits_{c} Sim_{f}^{(c)}$$</li></ol></li></ul><h3 id="10-3-个人总结"><a href="#10-3-个人总结" class="headerlink" title="10.3 个人总结"></a>10.3 个人总结</h3><div class="note note-info"><p>….</p></div></div><hr><div><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/metric-learning/" class="category-chain-item">metric-learning</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/paper/" class="print-no-link">#paper</a> <a href="/tags/metric-learning/" class="print-no-link">#metric-learning</a></div></div><div class="license-box my-3"><div class="license-title"><div>度量学习思路整合</div><div>http://paopaotangzu.xyz/cn/metric-learning-refinement/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>paopaotangzu</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2024年12月9日</div></div><div class="license-meta-item"><div>许可协议</div><div><a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"><a href="/cn/attention_mechanism/" title="注意力机制"><i class="iconfont icon-arrowleft"></i> <span class="hidden-mobile">注意力机制</span> <span class="visible-mobile">上一篇</span></a></article><article class="post-next col-6"><a href="/cn/ppt_technique/" title="PowerPoint技能点"><span class="hidden-mobile">PowerPoint技能点</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article id="comments" lazyload><div id="twikoo"></div><script type="text/javascript">Fluid.utils.loadComments("#comments",function(){Fluid.utils.createScript("https://lib.baomitu.com/twikoo/1.6.8/twikoo.all.min.js",function(){var o=Object.assign({envId:"https://twikoo.paopaotangzu.xyz/",region:"ap-shanghai",path:"window.location.pathname"},{el:"#twikoo",path:"window.location.pathname",onCommentLoaded:function(){Fluid.utils.listenDOMLoaded(function(){var o="#twikoo .tk-content img:not(.tk-owo-emotion)";Fluid.plugins.imageCaption(o),Fluid.plugins.fancyBox(o)})}});twikoo.init(o)})})</script><noscript>Please enable JavaScript to view the comments</noscript></article></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div></main><footer><div class="footer-inner"><div class="footer-content"><a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a><div style="font-size:.85rem"><span id="timeDate">载入天数...</span> <span id="times">载入时分秒...</span><script src="/js/duration.js"></script></div></div></div></footer><script src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js"></script><script src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js"></script><script>!function(t){var e=Fluid.plugins.typing,i=t.getElementById("subtitle");i&&e&&e(i.getAttribute("data-typed-text"))}((window,document))</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js",function(){var t,o=jQuery("#toc");0!==o.length&&window.tocbot&&(t=jQuery("#board-ctn").offset().top,window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-t},CONFIG.toc)),0<o.find(".toc-list-item").length&&o.css("visibility","visible"),Fluid.events.registerRefreshCallback(function(){if("tocbot"in window){tocbot.refresh();var t=jQuery("#toc");if(0===t.length||!tocbot)return;0<t.find(".toc-list-item").length&&t.css("visibility","visible")}}))})</script><script src="https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });</script><script>Fluid.utils.createScript("https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js",function(){Fluid.plugins.fancyBox()})</script><script>Fluid.plugins.imageCaption()</script><script>if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });</script><script src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js"></script><script defer src="/js/leancloud.js"></script><script src="/js/local-search.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div></noscript></body></html>